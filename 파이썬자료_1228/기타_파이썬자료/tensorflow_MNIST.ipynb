{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로 2.0의 가장 중요한 변화\n",
    "\n",
    "- 먼저 더 이상 \"케라스를 써야 하나요? 텐서플로를 써야 하나요?\"라는 질문은 의미 없다. 케라서는 이제 텐서플로의 일부이기 때문이다.\n",
    "\n",
    "- 또 다른 질무은 \"케라스를 서야 합니까? tf.keras를 써야 합니까?\"인데, tf.keras는 텐서플로 내부에 케라스를 구현해 놓은 것이다. 따라서 즉시(eager) 실행, tf.data와 다른 이점 등 다른 텐서플로 API와의 좀 더 나은 통합을 원한다면 케라스 대신 tf.keras를 써야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow 1.0과 2.0의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow 1.X 를 사용한 코드\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "in_a = tf.placeholder(dtype=tf.float32, shape=(2))\n",
    "in_b = tf.placeholder(dtype=tf.float32, shape=(2))\n",
    "\n",
    "def forward(x):\n",
    "  with tf.variable_scope(\"matmul\", reuse=tf.AUTO_REUSE):\n",
    "    W = tf.get_variable(\"W\", initializer=tf.ones(shape=(2,2)))\n",
    "    b = tf.get_variable(\"b\", initializer=tf.zeros(shape=(2)))\n",
    "    return W * x + b\n",
    "\n",
    "out_a = forward(in_a)\n",
    "out_b = forward(in_b)\n",
    "\n",
    "reg_loss = tf.losses.get_regularization_loss(scope=\"matmul\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  outs = sess.run([out_a, out_b, reg_loss],\n",
    "                feed_dict={in_a: [1, 0], in_b: [0, 1]})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변환 후 \n",
    "\n",
    "- 변수는 파이썬 지역 객체\n",
    "- forward 함수는 여전히 필요한 계산을 정의\n",
    "- Session.run 호출은 forward 함수를 호출하는 것으로 변경\n",
    "- tf.fucntion 데코레이터는 선택 사항으로 성능을 위해 추가 가능\n",
    "- 어떤 전역 컬렉션도 참조하지 않고 규제를 직접 계산\n",
    "- **세션이나 플레이스홀터를 사용하지 않음**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"StatefulPartitionedCall:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 2.0 을 사용한 코드\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "W = tf.Variable(tf.ones(shape=(2,2)), name=\"W\")\n",
    "b = tf.Variable(tf.zeros(shape=(2)), name=\"b\")\n",
    "\n",
    "@tf.function\n",
    "def forward(x):\n",
    "  return W * x + b\n",
    "\n",
    "out_a = forward([1,0])\n",
    "print(out_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"StatefulPartitionedCall_2:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "out_b = forward([0,1])\n",
    "print(out_b)\n",
    "regularizer = tf.keras.regularizers.l2(0.04)\n",
    "reg_loss = regularizer(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(reg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론\n",
    "\n",
    "#### 정의: 퍼셉트론은 입력 특징(feature) 또는 간단히 특징이라 불리는 n개의 크기를 갖는 입력 벡터 $(x_1, x_2, ..., x_n)$이 주어지면 1 또는 0을 출력하는 알고리즘\n",
    "\n",
    "#### 함수의 표현\n",
    "\n",
    "$$f(x)=\\left\\{\\begin{array}{cc} 1 & wx+b >0,\\\\ 0 & 그 외 \\end{array}\\right.$$\n",
    "\n",
    "#### Tensorflow 2.0 코드 구현\n",
    "\n",
    "- 붗꽃 데이터셋 - sklearn 라이브러리 이용\n",
    "\n",
    "- 일반적으로 tf.keras로 모델 작성하는 방법은 세 가지\n",
    "    - Sequential API\n",
    "    - Functional API\n",
    "    - Model Subclassing\n",
    "    \n",
    "- Sequential() 모델은 신경망 계층의 선형 pipeline\n",
    "    - 즉, 각 layer에 정확히 하나의 입력 텐서와 하나의 출력 텐서가 있는 일반 레이어 스택에 적합\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris=load_iris()\n",
    "X=iris.data[:, (2,3)] # 꽃잎의 길이와 너비\n",
    "y=(iris.target==0).astype(np.int) #부채붓꽃(Iris Setosa)인가?\n",
    "\n",
    "per_clf=Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred=per_clf.predict([[2,0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "NB_CLASSES=10\n",
    "RESHAPED = 784\n",
    "model=tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(NB_CLASSES, \n",
    "        input_shape=(RESHAPED,), kernel_initializer='zeros',\n",
    "        name='dense_layer',activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dense는 각 계층의 뉴런이 이전 계층에 위치한 모든 뉴런과 완전 연결돼 있고 그 다음 계층에 있는 모든 뉴런과도 완전 연결돼 있다는 것을 의미(fully-connected)\n",
    "- 각 뉴런은 kernel_initializer 매개변수를 통해 특정 가중치로 초기화할 수 있음.\n",
    "    - random_uniform\n",
    "    - random_normal\n",
    "    - zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 다층 퍼셉트론 구현\n",
    "\n",
    "- 텐서플로 2.0은 데이터셋을 로드하고 신경망을 미세 조정하는 훈련 집합 X_train으로 분할, 신경망의 성능을 평가하는 데 사용하는 테스트 집합 X_test로 분할하는 적절한 라이브러리를 제공\n",
    "- 데이터는 신경망을 훈련할 때 32비트 정밀도를 갖도록 float32로 변환되고 [0,1] 범위로 정규화됨\n",
    "- 실제 레이블을 각각 Y_train과 Y_test에 로드하고 원핫 인코딩을 수행\n",
    "- Sequential API를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 1.3825 - accuracy: 0.6712 - val_loss: 0.8894 - val_accuracy: 0.8297\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 0s 952us/step - loss: 0.7884 - accuracy: 0.8321 - val_loss: 0.6526 - val_accuracy: 0.8584\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 0s 894us/step - loss: 0.6396 - accuracy: 0.8520 - val_loss: 0.5582 - val_accuracy: 0.8686\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 0s 902us/step - loss: 0.5680 - accuracy: 0.8627 - val_loss: 0.5060 - val_accuracy: 0.8757\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 0s 923us/step - loss: 0.5244 - accuracy: 0.8690 - val_loss: 0.4725 - val_accuracy: 0.8819\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 0s 912us/step - loss: 0.4945 - accuracy: 0.8734 - val_loss: 0.4485 - val_accuracy: 0.8864\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 0s 936us/step - loss: 0.4724 - accuracy: 0.8779 - val_loss: 0.4305 - val_accuracy: 0.8898\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 0s 931us/step - loss: 0.4552 - accuracy: 0.8814 - val_loss: 0.4166 - val_accuracy: 0.8922\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 0s 949us/step - loss: 0.4414 - accuracy: 0.8839 - val_loss: 0.4052 - val_accuracy: 0.8947\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 0s 931us/step - loss: 0.4299 - accuracy: 0.8862 - val_loss: 0.3957 - val_accuracy: 0.8963\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 0s 926us/step - loss: 0.4202 - accuracy: 0.8874 - val_loss: 0.3877 - val_accuracy: 0.8983\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 0s 910us/step - loss: 0.4119 - accuracy: 0.8892 - val_loss: 0.3809 - val_accuracy: 0.9003\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 0s 926us/step - loss: 0.4046 - accuracy: 0.8904 - val_loss: 0.3748 - val_accuracy: 0.9018\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 0s 915us/step - loss: 0.3982 - accuracy: 0.8924 - val_loss: 0.3696 - val_accuracy: 0.9027\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 0s 947us/step - loss: 0.3925 - accuracy: 0.8935 - val_loss: 0.3650 - val_accuracy: 0.9032\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 0s 928us/step - loss: 0.3874 - accuracy: 0.8946 - val_loss: 0.3607 - val_accuracy: 0.9043\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 0s 923us/step - loss: 0.3827 - accuracy: 0.8960 - val_loss: 0.3570 - val_accuracy: 0.9048\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 0s 928us/step - loss: 0.3785 - accuracy: 0.8966 - val_loss: 0.3534 - val_accuracy: 0.9054\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 0s 926us/step - loss: 0.3746 - accuracy: 0.8976 - val_loss: 0.3502 - val_accuracy: 0.9069\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 0s 907us/step - loss: 0.3710 - accuracy: 0.8984 - val_loss: 0.3473 - val_accuracy: 0.9075\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 0s 918us/step - loss: 0.3677 - accuracy: 0.8989 - val_loss: 0.3446 - val_accuracy: 0.9077\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 0s 899us/step - loss: 0.3647 - accuracy: 0.8994 - val_loss: 0.3421 - val_accuracy: 0.9085\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 0s 904us/step - loss: 0.3618 - accuracy: 0.9002 - val_loss: 0.3398 - val_accuracy: 0.9091\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 0s 907us/step - loss: 0.3591 - accuracy: 0.9007 - val_loss: 0.3376 - val_accuracy: 0.9097\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 0s 915us/step - loss: 0.3566 - accuracy: 0.9014 - val_loss: 0.3355 - val_accuracy: 0.9102\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 0s 912us/step - loss: 0.3542 - accuracy: 0.9018 - val_loss: 0.3336 - val_accuracy: 0.9104\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 0s 926us/step - loss: 0.3520 - accuracy: 0.9025 - val_loss: 0.3318 - val_accuracy: 0.9109\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 0s 931us/step - loss: 0.3499 - accuracy: 0.9030 - val_loss: 0.3302 - val_accuracy: 0.9115\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 0s 926us/step - loss: 0.3479 - accuracy: 0.9033 - val_loss: 0.3285 - val_accuracy: 0.9116\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 0s 912us/step - loss: 0.3460 - accuracy: 0.9040 - val_loss: 0.3270 - val_accuracy: 0.9116\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 0s 920us/step - loss: 0.3441 - accuracy: 0.9043 - val_loss: 0.3255 - val_accuracy: 0.9119\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 0s 920us/step - loss: 0.3425 - accuracy: 0.9049 - val_loss: 0.3241 - val_accuracy: 0.9128\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 0s 915us/step - loss: 0.3408 - accuracy: 0.9053 - val_loss: 0.3228 - val_accuracy: 0.9126\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 0s 941us/step - loss: 0.3393 - accuracy: 0.9058 - val_loss: 0.3216 - val_accuracy: 0.9127\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 0s 928us/step - loss: 0.3378 - accuracy: 0.9061 - val_loss: 0.3205 - val_accuracy: 0.9132\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 0s 923us/step - loss: 0.3363 - accuracy: 0.9062 - val_loss: 0.3191 - val_accuracy: 0.9131\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 0s 957us/step - loss: 0.3349 - accuracy: 0.9068 - val_loss: 0.3181 - val_accuracy: 0.9132\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 0s 926us/step - loss: 0.3336 - accuracy: 0.9068 - val_loss: 0.3171 - val_accuracy: 0.9134\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 0s 941us/step - loss: 0.3324 - accuracy: 0.9073 - val_loss: 0.3160 - val_accuracy: 0.9133\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 0s 926us/step - loss: 0.3311 - accuracy: 0.9075 - val_loss: 0.3151 - val_accuracy: 0.9138\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 0s 912us/step - loss: 0.3300 - accuracy: 0.9078 - val_loss: 0.3141 - val_accuracy: 0.9143\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 0s 918us/step - loss: 0.3288 - accuracy: 0.9080 - val_loss: 0.3133 - val_accuracy: 0.9143\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 0s 926us/step - loss: 0.3277 - accuracy: 0.9084 - val_loss: 0.3124 - val_accuracy: 0.9146\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 0s 915us/step - loss: 0.3266 - accuracy: 0.9087 - val_loss: 0.3115 - val_accuracy: 0.9149\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 0s 912us/step - loss: 0.3256 - accuracy: 0.9087 - val_loss: 0.3107 - val_accuracy: 0.9149\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 0s 918us/step - loss: 0.3246 - accuracy: 0.9089 - val_loss: 0.3099 - val_accuracy: 0.9147\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 0s 904us/step - loss: 0.3236 - accuracy: 0.9091 - val_loss: 0.3091 - val_accuracy: 0.9148\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 0s 912us/step - loss: 0.3227 - accuracy: 0.9096 - val_loss: 0.3084 - val_accuracy: 0.9151\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 0s 928us/step - loss: 0.3218 - accuracy: 0.9099 - val_loss: 0.3078 - val_accuracy: 0.9154\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 0s 902us/step - loss: 0.3209 - accuracy: 0.9107 - val_loss: 0.3072 - val_accuracy: 0.9153\n",
      "313/313 [==============================] - 0s 398us/step - loss: 0.3068 - accuracy: 0.9156\n",
      "Test accuracy: 0.9156000018119812\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# network and training\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# loading MNIST dataset\n",
    "# verify\n",
    "# the split between train and test is 60,000, and 10,000 respectly \n",
    "# one-hot is automatically applied\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#normalize in [0,1]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "#one-hot\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "#build the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "   \t\tinput_shape=(RESHAPED,),\n",
    "   \t\tname='dense_layer', \n",
    "   \t\tactivation='softmax'))\n",
    "\n",
    "# summary of the model\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer='SGD',              # 기본값: lr=0.01\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For more details see at https://keras.io/losses, (or optimizers, or metrics)\n",
    "\n",
    "#training the model\n",
    "history=model.fit(X_train, Y_train,\n",
    "\t\tbatch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "\t\tverbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "#evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# making prediction\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 414us/step - loss: 0.3068 - accuracy: 0.9156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3068375587463379, 0.9156000018119812]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선1 -  Hidden layer 추가하기.\n",
    "\n",
    "- 위 신경망 모형의 test accuracysms 91.65% \n",
    "- 여기에 128개 뉴런을 갖는 hidden layer 추가하면?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 1.4381 - accuracy: 0.6323 - val_loss: 0.7079 - val_accuracy: 0.8489\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.5643 - accuracy: 0.8625 - val_loss: 0.4322 - val_accuracy: 0.8917\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4187 - accuracy: 0.8866 - val_loss: 0.3621 - val_accuracy: 0.9027\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3656 - accuracy: 0.8979 - val_loss: 0.3266 - val_accuracy: 0.9105\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3353 - accuracy: 0.9049 - val_loss: 0.3047 - val_accuracy: 0.9156\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3137 - accuracy: 0.9104 - val_loss: 0.2884 - val_accuracy: 0.9194\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.9153 - val_loss: 0.2764 - val_accuracy: 0.9227\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2824 - accuracy: 0.9188 - val_loss: 0.2664 - val_accuracy: 0.9226\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2702 - accuracy: 0.9233 - val_loss: 0.2540 - val_accuracy: 0.9264\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2591 - accuracy: 0.9257 - val_loss: 0.2474 - val_accuracy: 0.9287\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2495 - accuracy: 0.9283 - val_loss: 0.2363 - val_accuracy: 0.9311\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.9317 - val_loss: 0.2308 - val_accuracy: 0.9334\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9333 - val_loss: 0.2224 - val_accuracy: 0.9355\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2246 - accuracy: 0.9360 - val_loss: 0.2164 - val_accuracy: 0.9391\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2176 - accuracy: 0.9380 - val_loss: 0.2109 - val_accuracy: 0.9407\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2109 - accuracy: 0.9396 - val_loss: 0.2064 - val_accuracy: 0.9417\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2047 - accuracy: 0.9412 - val_loss: 0.2019 - val_accuracy: 0.9422\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9429 - val_loss: 0.1970 - val_accuracy: 0.9436\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9444 - val_loss: 0.1918 - val_accuracy: 0.9468\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9464 - val_loss: 0.1882 - val_accuracy: 0.9466\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9472 - val_loss: 0.1852 - val_accuracy: 0.9465\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1785 - accuracy: 0.9489 - val_loss: 0.1806 - val_accuracy: 0.9494\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9506 - val_loss: 0.1761 - val_accuracy: 0.9507\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9516 - val_loss: 0.1734 - val_accuracy: 0.9514\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9526 - val_loss: 0.1701 - val_accuracy: 0.9519\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9538 - val_loss: 0.1670 - val_accuracy: 0.9530\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9547 - val_loss: 0.1652 - val_accuracy: 0.9539\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1549 - accuracy: 0.9558 - val_loss: 0.1615 - val_accuracy: 0.9541\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9564 - val_loss: 0.1602 - val_accuracy: 0.9554\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9578 - val_loss: 0.1570 - val_accuracy: 0.9567\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9581 - val_loss: 0.1544 - val_accuracy: 0.9564\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9589 - val_loss: 0.1531 - val_accuracy: 0.9563\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9601 - val_loss: 0.1508 - val_accuracy: 0.9582\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9609 - val_loss: 0.1497 - val_accuracy: 0.9588\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9617 - val_loss: 0.1464 - val_accuracy: 0.9597\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9624 - val_loss: 0.1451 - val_accuracy: 0.9603\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9632 - val_loss: 0.1446 - val_accuracy: 0.9599\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9640 - val_loss: 0.1416 - val_accuracy: 0.9607\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9650 - val_loss: 0.1401 - val_accuracy: 0.9611\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9653 - val_loss: 0.1382 - val_accuracy: 0.9618\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9660 - val_loss: 0.1367 - val_accuracy: 0.9620\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9671 - val_loss: 0.1352 - val_accuracy: 0.9623\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9670 - val_loss: 0.1334 - val_accuracy: 0.9624\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9679 - val_loss: 0.1324 - val_accuracy: 0.9627\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9684 - val_loss: 0.1308 - val_accuracy: 0.9632\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9690 - val_loss: 0.1291 - val_accuracy: 0.9633\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9697 - val_loss: 0.1293 - val_accuracy: 0.9631\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9701 - val_loss: 0.1268 - val_accuracy: 0.9643\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9703 - val_loss: 0.1258 - val_accuracy: 0.9643\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9713 - val_loss: 0.1251 - val_accuracy: 0.9645\n",
      "313/313 [==============================] - 0s 498us/step - loss: 0.1199 - accuracy: 0.9639\n",
      "Test accuracy: 0.9639000296592712\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# network and training\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# loading MNIST dataset\n",
    "# verify\n",
    "# the split between train and test is 60,000, and 10,000 respectly \n",
    "# one-hot is automatically applied\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#normalize in [0,1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "#one-hot\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "#build the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "   \t\tinput_shape=(RESHAPED,),\n",
    "   \t\tname='dense_layer', activation='relu'))\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "   \t\tname='dense_layer_2', activation='relu'))\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "   \t\tname='dense_layer_3', activation='softmax'))\n",
    "\n",
    "\n",
    "# build the model - alternative method\n",
    "\n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Dense(300, activation=\"relu\"),\n",
    "#     keras.layers.Dense(100, activation=\"relu\"),\n",
    "#     keras.layers.Dense(10, activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# summary of the model\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer='SGD', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#training the model\n",
    "model.fit(X_train, Y_train,\n",
    "\t\tbatch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "\t\tverbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "#evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# making prediction\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선 2 - 드랍아웃\n",
    "\n",
    "- 2개의 hidden layer를 추가하면 test accuracy가 96.39%로 증가\n",
    "- 여기에 dropout을 사용하면?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 1.6901 - accuracy: 0.4683 - val_loss: 0.8584 - val_accuracy: 0.8156\n",
      "Epoch 2/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.8874 - accuracy: 0.7290 - val_loss: 0.5142 - val_accuracy: 0.8684\n",
      "Epoch 3/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.6758 - accuracy: 0.7926 - val_loss: 0.4142 - val_accuracy: 0.8898\n",
      "Epoch 4/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.5794 - accuracy: 0.8231 - val_loss: 0.3652 - val_accuracy: 0.8998\n",
      "Epoch 5/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.5161 - accuracy: 0.8438 - val_loss: 0.3323 - val_accuracy: 0.9074\n",
      "Epoch 6/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4786 - accuracy: 0.8569 - val_loss: 0.3113 - val_accuracy: 0.9111\n",
      "Epoch 7/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4469 - accuracy: 0.8673 - val_loss: 0.2931 - val_accuracy: 0.9168\n",
      "Epoch 8/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4196 - accuracy: 0.8750 - val_loss: 0.2779 - val_accuracy: 0.9179\n",
      "Epoch 9/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4020 - accuracy: 0.8813 - val_loss: 0.2651 - val_accuracy: 0.9218\n",
      "Epoch 10/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8860 - val_loss: 0.2545 - val_accuracy: 0.9252\n",
      "Epoch 11/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.8915 - val_loss: 0.2440 - val_accuracy: 0.9277\n",
      "Epoch 12/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3543 - accuracy: 0.8969 - val_loss: 0.2373 - val_accuracy: 0.9310\n",
      "Epoch 13/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.9003 - val_loss: 0.2281 - val_accuracy: 0.9330\n",
      "Epoch 14/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.9019 - val_loss: 0.2211 - val_accuracy: 0.9352\n",
      "Epoch 15/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3202 - accuracy: 0.9050 - val_loss: 0.2150 - val_accuracy: 0.9381\n",
      "Epoch 16/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3114 - accuracy: 0.9083 - val_loss: 0.2094 - val_accuracy: 0.9392\n",
      "Epoch 17/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3028 - accuracy: 0.9103 - val_loss: 0.2046 - val_accuracy: 0.9411\n",
      "Epoch 18/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.9132 - val_loss: 0.1980 - val_accuracy: 0.9433\n",
      "Epoch 19/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2881 - accuracy: 0.9155 - val_loss: 0.1941 - val_accuracy: 0.9438\n",
      "Epoch 20/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2795 - accuracy: 0.9175 - val_loss: 0.1887 - val_accuracy: 0.9459\n",
      "Epoch 21/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.9199 - val_loss: 0.1842 - val_accuracy: 0.9460\n",
      "Epoch 22/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2676 - accuracy: 0.9212 - val_loss: 0.1805 - val_accuracy: 0.9489\n",
      "Epoch 23/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2637 - accuracy: 0.9222 - val_loss: 0.1768 - val_accuracy: 0.9490\n",
      "Epoch 24/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2556 - accuracy: 0.9242 - val_loss: 0.1736 - val_accuracy: 0.9502\n",
      "Epoch 25/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.9247 - val_loss: 0.1708 - val_accuracy: 0.9513\n",
      "Epoch 26/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2428 - accuracy: 0.9282 - val_loss: 0.1668 - val_accuracy: 0.9513\n",
      "Epoch 27/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2429 - accuracy: 0.9290 - val_loss: 0.1642 - val_accuracy: 0.9526\n",
      "Epoch 28/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2391 - accuracy: 0.9296 - val_loss: 0.1609 - val_accuracy: 0.9533\n",
      "Epoch 29/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.9314 - val_loss: 0.1584 - val_accuracy: 0.9536\n",
      "Epoch 30/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2304 - accuracy: 0.9324 - val_loss: 0.1554 - val_accuracy: 0.9547\n",
      "Epoch 31/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2252 - accuracy: 0.9339 - val_loss: 0.1536 - val_accuracy: 0.9554\n",
      "Epoch 32/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2223 - accuracy: 0.9352 - val_loss: 0.1519 - val_accuracy: 0.9557\n",
      "Epoch 33/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2154 - accuracy: 0.9373 - val_loss: 0.1489 - val_accuracy: 0.9563\n",
      "Epoch 34/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.9368 - val_loss: 0.1476 - val_accuracy: 0.9567\n",
      "Epoch 35/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2126 - accuracy: 0.9376 - val_loss: 0.1448 - val_accuracy: 0.9578\n",
      "Epoch 36/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2086 - accuracy: 0.9371 - val_loss: 0.1431 - val_accuracy: 0.9585\n",
      "Epoch 37/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9379 - val_loss: 0.1415 - val_accuracy: 0.9588\n",
      "Epoch 38/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2038 - accuracy: 0.9406 - val_loss: 0.1392 - val_accuracy: 0.9601\n",
      "Epoch 39/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9409 - val_loss: 0.1375 - val_accuracy: 0.9603\n",
      "Epoch 40/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1971 - accuracy: 0.9425 - val_loss: 0.1359 - val_accuracy: 0.9607\n",
      "Epoch 41/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9416 - val_loss: 0.1342 - val_accuracy: 0.9609\n",
      "Epoch 42/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1960 - accuracy: 0.9423 - val_loss: 0.1330 - val_accuracy: 0.9614\n",
      "Epoch 43/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9437 - val_loss: 0.1313 - val_accuracy: 0.9620\n",
      "Epoch 44/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1865 - accuracy: 0.9452 - val_loss: 0.1311 - val_accuracy: 0.9628\n",
      "Epoch 45/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9447 - val_loss: 0.1281 - val_accuracy: 0.9634\n",
      "Epoch 46/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9453 - val_loss: 0.1273 - val_accuracy: 0.9634\n",
      "Epoch 47/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9455 - val_loss: 0.1260 - val_accuracy: 0.9646\n",
      "Epoch 48/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9469 - val_loss: 0.1248 - val_accuracy: 0.9647\n",
      "Epoch 49/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9472 - val_loss: 0.1238 - val_accuracy: 0.9645\n",
      "Epoch 50/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9470 - val_loss: 0.1221 - val_accuracy: 0.9647\n",
      "Epoch 51/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9492 - val_loss: 0.1214 - val_accuracy: 0.9652\n",
      "Epoch 52/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9498 - val_loss: 0.1203 - val_accuracy: 0.9652\n",
      "Epoch 53/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9500 - val_loss: 0.1196 - val_accuracy: 0.9659\n",
      "Epoch 54/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9504 - val_loss: 0.1188 - val_accuracy: 0.9658\n",
      "Epoch 55/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9516 - val_loss: 0.1181 - val_accuracy: 0.9663\n",
      "Epoch 56/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 0.9507 - val_loss: 0.1165 - val_accuracy: 0.9662\n",
      "Epoch 57/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9516 - val_loss: 0.1158 - val_accuracy: 0.9670\n",
      "Epoch 58/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9524 - val_loss: 0.1150 - val_accuracy: 0.9665\n",
      "Epoch 59/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9524 - val_loss: 0.1137 - val_accuracy: 0.9668\n",
      "Epoch 60/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9527 - val_loss: 0.1132 - val_accuracy: 0.9672\n",
      "Epoch 61/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9536 - val_loss: 0.1122 - val_accuracy: 0.9676\n",
      "Epoch 62/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9529 - val_loss: 0.1111 - val_accuracy: 0.9676\n",
      "Epoch 63/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9529 - val_loss: 0.1109 - val_accuracy: 0.9681\n",
      "Epoch 64/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9542 - val_loss: 0.1100 - val_accuracy: 0.9676\n",
      "Epoch 65/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9551 - val_loss: 0.1093 - val_accuracy: 0.9681\n",
      "Epoch 66/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9548 - val_loss: 0.1093 - val_accuracy: 0.9684\n",
      "Epoch 67/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9569 - val_loss: 0.1085 - val_accuracy: 0.9687\n",
      "Epoch 68/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9570 - val_loss: 0.1069 - val_accuracy: 0.9688\n",
      "Epoch 69/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9565 - val_loss: 0.1071 - val_accuracy: 0.9689\n",
      "Epoch 70/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9561 - val_loss: 0.1065 - val_accuracy: 0.9693\n",
      "Epoch 71/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9569 - val_loss: 0.1057 - val_accuracy: 0.9694\n",
      "Epoch 72/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9567 - val_loss: 0.1049 - val_accuracy: 0.9697\n",
      "Epoch 73/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9580 - val_loss: 0.1044 - val_accuracy: 0.9693\n",
      "Epoch 74/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9595 - val_loss: 0.1038 - val_accuracy: 0.9699\n",
      "Epoch 75/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9592 - val_loss: 0.1030 - val_accuracy: 0.9694\n",
      "Epoch 76/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9585 - val_loss: 0.1029 - val_accuracy: 0.9698\n",
      "Epoch 77/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9602 - val_loss: 0.1020 - val_accuracy: 0.9692\n",
      "Epoch 78/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9597 - val_loss: 0.1016 - val_accuracy: 0.9702\n",
      "Epoch 79/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9613 - val_loss: 0.1014 - val_accuracy: 0.9702\n",
      "Epoch 80/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9606 - val_loss: 0.1007 - val_accuracy: 0.9707\n",
      "Epoch 81/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9605 - val_loss: 0.1008 - val_accuracy: 0.9699\n",
      "Epoch 82/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9600 - val_loss: 0.1000 - val_accuracy: 0.9702\n",
      "Epoch 83/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9597 - val_loss: 0.0994 - val_accuracy: 0.9707\n",
      "Epoch 84/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9609 - val_loss: 0.0985 - val_accuracy: 0.9709\n",
      "Epoch 85/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9623 - val_loss: 0.0985 - val_accuracy: 0.9707\n",
      "Epoch 86/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9615 - val_loss: 0.0977 - val_accuracy: 0.9717\n",
      "Epoch 87/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9613 - val_loss: 0.0973 - val_accuracy: 0.9707\n",
      "Epoch 88/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9618 - val_loss: 0.0968 - val_accuracy: 0.9718\n",
      "Epoch 89/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9621 - val_loss: 0.0965 - val_accuracy: 0.9718\n",
      "Epoch 90/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9626 - val_loss: 0.0961 - val_accuracy: 0.9714\n",
      "Epoch 91/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9628 - val_loss: 0.0956 - val_accuracy: 0.9715\n",
      "Epoch 92/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9624 - val_loss: 0.0959 - val_accuracy: 0.9714\n",
      "Epoch 93/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9635 - val_loss: 0.0949 - val_accuracy: 0.9718\n",
      "Epoch 94/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9637 - val_loss: 0.0952 - val_accuracy: 0.9716\n",
      "Epoch 95/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9628 - val_loss: 0.0947 - val_accuracy: 0.9720\n",
      "Epoch 96/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9631 - val_loss: 0.0940 - val_accuracy: 0.9720\n",
      "Epoch 97/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9650 - val_loss: 0.0936 - val_accuracy: 0.9719\n",
      "Epoch 98/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9650 - val_loss: 0.0931 - val_accuracy: 0.9715\n",
      "Epoch 99/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9646 - val_loss: 0.0934 - val_accuracy: 0.9722\n",
      "Epoch 100/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9650 - val_loss: 0.0925 - val_accuracy: 0.9722\n",
      "Epoch 101/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9648 - val_loss: 0.0929 - val_accuracy: 0.9728\n",
      "Epoch 102/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9651 - val_loss: 0.0920 - val_accuracy: 0.9725\n",
      "Epoch 103/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9647 - val_loss: 0.0922 - val_accuracy: 0.9727\n",
      "Epoch 104/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9662 - val_loss: 0.0917 - val_accuracy: 0.9728\n",
      "Epoch 105/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9662 - val_loss: 0.0916 - val_accuracy: 0.9722\n",
      "Epoch 106/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9665 - val_loss: 0.0912 - val_accuracy: 0.9729\n",
      "Epoch 107/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9667 - val_loss: 0.0909 - val_accuracy: 0.9732\n",
      "Epoch 108/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9653 - val_loss: 0.0907 - val_accuracy: 0.9727\n",
      "Epoch 109/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9664 - val_loss: 0.0904 - val_accuracy: 0.9725\n",
      "Epoch 110/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9668 - val_loss: 0.0900 - val_accuracy: 0.9728\n",
      "Epoch 111/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9667 - val_loss: 0.0898 - val_accuracy: 0.9728\n",
      "Epoch 112/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9667 - val_loss: 0.0896 - val_accuracy: 0.9731\n",
      "Epoch 113/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9678 - val_loss: 0.0894 - val_accuracy: 0.9727\n",
      "Epoch 114/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9676 - val_loss: 0.0889 - val_accuracy: 0.9729\n",
      "Epoch 115/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9673 - val_loss: 0.0884 - val_accuracy: 0.9735\n",
      "Epoch 116/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9693 - val_loss: 0.0885 - val_accuracy: 0.9732\n",
      "Epoch 117/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9688 - val_loss: 0.0875 - val_accuracy: 0.9739\n",
      "Epoch 118/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9683 - val_loss: 0.0879 - val_accuracy: 0.9739\n",
      "Epoch 119/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9679 - val_loss: 0.0877 - val_accuracy: 0.9735\n",
      "Epoch 120/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9676 - val_loss: 0.0875 - val_accuracy: 0.9737\n",
      "Epoch 121/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9688 - val_loss: 0.0871 - val_accuracy: 0.9738\n",
      "Epoch 122/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9688 - val_loss: 0.0873 - val_accuracy: 0.9733\n",
      "Epoch 123/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9689 - val_loss: 0.0867 - val_accuracy: 0.9735\n",
      "Epoch 124/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9688 - val_loss: 0.0869 - val_accuracy: 0.9743\n",
      "Epoch 125/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9708 - val_loss: 0.0867 - val_accuracy: 0.9744\n",
      "Epoch 126/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9691 - val_loss: 0.0867 - val_accuracy: 0.9737\n",
      "Epoch 127/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9697 - val_loss: 0.0861 - val_accuracy: 0.9737\n",
      "Epoch 128/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9701 - val_loss: 0.0859 - val_accuracy: 0.9747\n",
      "Epoch 129/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9686 - val_loss: 0.0853 - val_accuracy: 0.9751\n",
      "Epoch 130/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9694 - val_loss: 0.0859 - val_accuracy: 0.9738\n",
      "Epoch 131/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9706 - val_loss: 0.0851 - val_accuracy: 0.9743\n",
      "Epoch 132/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9702 - val_loss: 0.0858 - val_accuracy: 0.9745\n",
      "Epoch 133/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9708 - val_loss: 0.0852 - val_accuracy: 0.9743\n",
      "Epoch 134/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9712 - val_loss: 0.0848 - val_accuracy: 0.9743\n",
      "Epoch 135/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9707 - val_loss: 0.0847 - val_accuracy: 0.9745\n",
      "Epoch 136/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9696 - val_loss: 0.0845 - val_accuracy: 0.9753\n",
      "Epoch 137/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9708 - val_loss: 0.0840 - val_accuracy: 0.9746\n",
      "Epoch 138/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9706 - val_loss: 0.0843 - val_accuracy: 0.9748\n",
      "Epoch 139/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9708 - val_loss: 0.0843 - val_accuracy: 0.9750\n",
      "Epoch 140/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9706 - val_loss: 0.0842 - val_accuracy: 0.9750\n",
      "Epoch 141/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9714 - val_loss: 0.0839 - val_accuracy: 0.9748\n",
      "Epoch 142/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9712 - val_loss: 0.0843 - val_accuracy: 0.9751\n",
      "Epoch 143/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9726 - val_loss: 0.0834 - val_accuracy: 0.9754\n",
      "Epoch 144/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9716 - val_loss: 0.0834 - val_accuracy: 0.9754\n",
      "Epoch 145/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9720 - val_loss: 0.0840 - val_accuracy: 0.9758\n",
      "Epoch 146/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9725 - val_loss: 0.0831 - val_accuracy: 0.9754\n",
      "Epoch 147/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9720 - val_loss: 0.0833 - val_accuracy: 0.9757\n",
      "Epoch 148/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.9720 - val_loss: 0.0826 - val_accuracy: 0.9755\n",
      "Epoch 149/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9722 - val_loss: 0.0824 - val_accuracy: 0.9761\n",
      "Epoch 150/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9726 - val_loss: 0.0835 - val_accuracy: 0.9756\n",
      "Epoch 151/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9722 - val_loss: 0.0821 - val_accuracy: 0.9756\n",
      "Epoch 152/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9724 - val_loss: 0.0821 - val_accuracy: 0.9758\n",
      "Epoch 153/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9720 - val_loss: 0.0825 - val_accuracy: 0.9752\n",
      "Epoch 154/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9728 - val_loss: 0.0818 - val_accuracy: 0.9753\n",
      "Epoch 155/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9739 - val_loss: 0.0819 - val_accuracy: 0.9761\n",
      "Epoch 156/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9731 - val_loss: 0.0821 - val_accuracy: 0.9758\n",
      "Epoch 157/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9727 - val_loss: 0.0818 - val_accuracy: 0.9761\n",
      "Epoch 158/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9746 - val_loss: 0.0818 - val_accuracy: 0.9761\n",
      "Epoch 159/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9729 - val_loss: 0.0817 - val_accuracy: 0.9765\n",
      "Epoch 160/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9741 - val_loss: 0.0821 - val_accuracy: 0.9762\n",
      "Epoch 161/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9736 - val_loss: 0.0814 - val_accuracy: 0.9761\n",
      "Epoch 162/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9723 - val_loss: 0.0818 - val_accuracy: 0.9758\n",
      "Epoch 163/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9729 - val_loss: 0.0814 - val_accuracy: 0.9766\n",
      "Epoch 164/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9748 - val_loss: 0.0815 - val_accuracy: 0.9755\n",
      "Epoch 165/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9739 - val_loss: 0.0820 - val_accuracy: 0.9767\n",
      "Epoch 166/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9753 - val_loss: 0.0807 - val_accuracy: 0.9764\n",
      "Epoch 167/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9746 - val_loss: 0.0813 - val_accuracy: 0.9766\n",
      "Epoch 168/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9746 - val_loss: 0.0812 - val_accuracy: 0.9764\n",
      "Epoch 169/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9745 - val_loss: 0.0809 - val_accuracy: 0.9765\n",
      "Epoch 170/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9747 - val_loss: 0.0804 - val_accuracy: 0.9766\n",
      "Epoch 171/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.9748 - val_loss: 0.0807 - val_accuracy: 0.9763\n",
      "Epoch 172/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0816 - accuracy: 0.9739 - val_loss: 0.0804 - val_accuracy: 0.9766\n",
      "Epoch 173/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9740 - val_loss: 0.0808 - val_accuracy: 0.9762\n",
      "Epoch 174/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9749 - val_loss: 0.0803 - val_accuracy: 0.9762\n",
      "Epoch 175/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9756 - val_loss: 0.0805 - val_accuracy: 0.9769\n",
      "Epoch 176/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9751 - val_loss: 0.0804 - val_accuracy: 0.9767\n",
      "Epoch 177/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9750 - val_loss: 0.0806 - val_accuracy: 0.9760\n",
      "Epoch 178/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9762 - val_loss: 0.0804 - val_accuracy: 0.9768\n",
      "Epoch 179/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9744 - val_loss: 0.0802 - val_accuracy: 0.9762\n",
      "Epoch 180/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9764 - val_loss: 0.0799 - val_accuracy: 0.9764\n",
      "Epoch 181/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9761 - val_loss: 0.0798 - val_accuracy: 0.9767\n",
      "Epoch 182/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9758 - val_loss: 0.0796 - val_accuracy: 0.9767\n",
      "Epoch 183/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9758 - val_loss: 0.0793 - val_accuracy: 0.9768\n",
      "Epoch 184/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9761 - val_loss: 0.0799 - val_accuracy: 0.9766\n",
      "Epoch 185/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9759 - val_loss: 0.0798 - val_accuracy: 0.9764\n",
      "Epoch 186/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9766 - val_loss: 0.0795 - val_accuracy: 0.9765\n",
      "Epoch 187/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9757 - val_loss: 0.0792 - val_accuracy: 0.9770\n",
      "Epoch 188/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9769 - val_loss: 0.0798 - val_accuracy: 0.9770\n",
      "Epoch 189/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9761 - val_loss: 0.0789 - val_accuracy: 0.9773\n",
      "Epoch 190/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9756 - val_loss: 0.0788 - val_accuracy: 0.9770\n",
      "Epoch 191/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9766 - val_loss: 0.0788 - val_accuracy: 0.9770\n",
      "Epoch 192/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.9764 - val_loss: 0.0791 - val_accuracy: 0.9768\n",
      "Epoch 193/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9780 - val_loss: 0.0789 - val_accuracy: 0.9768\n",
      "Epoch 194/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9764 - val_loss: 0.0788 - val_accuracy: 0.9766\n",
      "Epoch 195/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9764 - val_loss: 0.0787 - val_accuracy: 0.9766\n",
      "Epoch 196/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9773 - val_loss: 0.0789 - val_accuracy: 0.9770\n",
      "Epoch 197/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9767 - val_loss: 0.0785 - val_accuracy: 0.9777\n",
      "Epoch 198/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9764 - val_loss: 0.0784 - val_accuracy: 0.9773\n",
      "Epoch 199/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.9774 - val_loss: 0.0783 - val_accuracy: 0.9774\n",
      "Epoch 200/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.9777 - val_loss: 0.0782 - val_accuracy: 0.9768\n",
      "313/313 [==============================] - 0s 492us/step - loss: 0.0738 - accuracy: 0.9774\n",
      "Test accuracy: 0.977400004863739\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# network and training\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# loading MNIST dataset\n",
    "# verify\n",
    "# the split between train and test is 60,000, and 10,000 respectly \n",
    "# one-hot is automatically applied\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#normalize in [0,1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "#one-hot\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "#build the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "   \t\tinput_shape=(RESHAPED,),\n",
    "   \t\tname='dense_layer', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "   \t\tname='dense_layer_2', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "   \t\tname='dense_layer_3', activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# summary of the model\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer='SGD', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#training the moodel\n",
    "model.fit(X_train, Y_train,\n",
    "\t\tbatch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "\t\tverbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "#evalute the model\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# making prediction\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선 3 - 여러 최적화기 사용하기\n",
    "\n",
    "- dropout을 사용하면 test accuracy가 97.74%로 증가\n",
    "- 먼저 'RMSprop' 최적기를 사용하면?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "  1/375 [..............................] - ETA: 0s - loss: 2.4098 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0324s). Check your callbacks.\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4737 - accuracy: 0.8568 - val_loss: 0.1854 - val_accuracy: 0.9452\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2294 - accuracy: 0.9324 - val_loss: 0.1348 - val_accuracy: 0.9596\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.9485 - val_loss: 0.1225 - val_accuracy: 0.9640\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1480 - accuracy: 0.9555 - val_loss: 0.1087 - val_accuracy: 0.9666\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1315 - accuracy: 0.9603 - val_loss: 0.1001 - val_accuracy: 0.9715\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1223 - accuracy: 0.9638 - val_loss: 0.0990 - val_accuracy: 0.9706\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.9673 - val_loss: 0.0923 - val_accuracy: 0.9743\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1023 - accuracy: 0.9693 - val_loss: 0.0917 - val_accuracy: 0.9742\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0970 - accuracy: 0.9707 - val_loss: 0.0950 - val_accuracy: 0.9745\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0907 - accuracy: 0.9729 - val_loss: 0.0912 - val_accuracy: 0.9762\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0922 - accuracy: 0.9739 - val_loss: 0.0921 - val_accuracy: 0.9768\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0850 - accuracy: 0.9743 - val_loss: 0.0929 - val_accuracy: 0.9758\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0828 - accuracy: 0.9754 - val_loss: 0.0990 - val_accuracy: 0.9753\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0805 - accuracy: 0.9769 - val_loss: 0.0992 - val_accuracy: 0.9764\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0771 - accuracy: 0.9778 - val_loss: 0.0995 - val_accuracy: 0.9760\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0758 - accuracy: 0.9779 - val_loss: 0.1052 - val_accuracy: 0.9772\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0746 - accuracy: 0.9789 - val_loss: 0.0994 - val_accuracy: 0.9777\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0736 - accuracy: 0.9786 - val_loss: 0.0995 - val_accuracy: 0.9789\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0698 - accuracy: 0.9792 - val_loss: 0.1074 - val_accuracy: 0.9768\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0692 - accuracy: 0.9798 - val_loss: 0.1081 - val_accuracy: 0.9778\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0697 - accuracy: 0.9802 - val_loss: 0.1060 - val_accuracy: 0.9786\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0685 - accuracy: 0.9798 - val_loss: 0.1051 - val_accuracy: 0.9784\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0631 - accuracy: 0.9817 - val_loss: 0.1097 - val_accuracy: 0.9775\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0607 - accuracy: 0.9820 - val_loss: 0.1086 - val_accuracy: 0.9787\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0625 - accuracy: 0.9826 - val_loss: 0.1117 - val_accuracy: 0.9772\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0620 - accuracy: 0.9825 - val_loss: 0.1165 - val_accuracy: 0.9768\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0627 - accuracy: 0.9823 - val_loss: 0.1127 - val_accuracy: 0.9788\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0578 - accuracy: 0.9834 - val_loss: 0.1135 - val_accuracy: 0.9777\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0602 - accuracy: 0.9832 - val_loss: 0.1131 - val_accuracy: 0.9792\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0576 - accuracy: 0.9835 - val_loss: 0.1163 - val_accuracy: 0.9787\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0585 - accuracy: 0.9836 - val_loss: 0.1296 - val_accuracy: 0.9778\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0610 - accuracy: 0.9820 - val_loss: 0.1178 - val_accuracy: 0.9783\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0601 - accuracy: 0.9831 - val_loss: 0.1189 - val_accuracy: 0.9781\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0565 - accuracy: 0.9839 - val_loss: 0.1199 - val_accuracy: 0.9787\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0575 - accuracy: 0.9842 - val_loss: 0.1150 - val_accuracy: 0.9791\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0528 - accuracy: 0.9845 - val_loss: 0.1216 - val_accuracy: 0.9791\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0550 - accuracy: 0.9846 - val_loss: 0.1291 - val_accuracy: 0.9773\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0547 - accuracy: 0.9847 - val_loss: 0.1243 - val_accuracy: 0.9793\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0582 - accuracy: 0.9839 - val_loss: 0.1262 - val_accuracy: 0.9802\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0514 - accuracy: 0.9853 - val_loss: 0.1415 - val_accuracy: 0.9787\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0553 - accuracy: 0.9849 - val_loss: 0.1297 - val_accuracy: 0.9794\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0509 - accuracy: 0.9858 - val_loss: 0.1376 - val_accuracy: 0.9793\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0567 - accuracy: 0.9849 - val_loss: 0.1349 - val_accuracy: 0.9802\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0566 - accuracy: 0.9846 - val_loss: 0.1359 - val_accuracy: 0.9795\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0556 - accuracy: 0.9854 - val_loss: 0.1391 - val_accuracy: 0.9793\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0530 - accuracy: 0.9855 - val_loss: 0.1557 - val_accuracy: 0.9778\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0509 - accuracy: 0.9854 - val_loss: 0.1433 - val_accuracy: 0.9793\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0499 - accuracy: 0.9864 - val_loss: 0.1479 - val_accuracy: 0.9788\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0535 - accuracy: 0.9856 - val_loss: 0.1454 - val_accuracy: 0.9803\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.0531 - accuracy: 0.9862 - val_loss: 0.1482 - val_accuracy: 0.9787\n",
      "313/313 [==============================] - 0s 484us/step - loss: 0.1358 - accuracy: 0.9788\n",
      "Test accuracy: 0.9787999987602234\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "#for tensorboard\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard_callback = TensorBoard('.logdir')\n",
    "\n",
    "# network and training\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# loading MNIST dataset\n",
    "# verify\n",
    "# the split between train and test is 60,000, and 10,000 respectly \n",
    "# one-hot is automatically applied\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#normalize in [0,1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "#one-hot\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "#build the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "   \t\tinput_shape=(RESHAPED,),\n",
    "   \t\tname='dense_layer', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "   \t\tname='dense_layer_2', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "   \t\tname='dense_layer_3', activation='softmax'))\n",
    "\n",
    "# summary of the model\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer='RMSProp', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#training the moodel\n",
    "model.fit(X_train, Y_train,\n",
    "\t\tbatch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "\t\tverbose=VERBOSE, validation_split=VALIDATION_SPLIT,\n",
    "\t\tcallbacks=[tensorboard_callback])\n",
    "\n",
    "#evalute the model\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# making prediction\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Adam' 최적기를 사용하면?\n",
    "\n",
    "- RMSprop (50 에포크)을 사용하면 97.88% 의 test_accuracy를 얻을 수 있었다.\n",
    "- 'Adam'을 사용하면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "  1/375 [..............................] - ETA: 0s - loss: 2.4976 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0260s). Check your callbacks.\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5302 - accuracy: 0.8400 - val_loss: 0.1870 - val_accuracy: 0.9451\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2307 - accuracy: 0.9316 - val_loss: 0.1373 - val_accuracy: 0.9588\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.9466 - val_loss: 0.1171 - val_accuracy: 0.9653\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9547 - val_loss: 0.1021 - val_accuracy: 0.9699\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9612 - val_loss: 0.0921 - val_accuracy: 0.9736\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9647 - val_loss: 0.0879 - val_accuracy: 0.9733\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9676 - val_loss: 0.0854 - val_accuracy: 0.9747\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9701 - val_loss: 0.0876 - val_accuracy: 0.9759\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9727 - val_loss: 0.0849 - val_accuracy: 0.9753\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9731 - val_loss: 0.0791 - val_accuracy: 0.9775\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9754 - val_loss: 0.0792 - val_accuracy: 0.9760\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9751 - val_loss: 0.0775 - val_accuracy: 0.9784\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.9776 - val_loss: 0.0793 - val_accuracy: 0.9794\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.9795 - val_loss: 0.0788 - val_accuracy: 0.9778\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.9795 - val_loss: 0.0786 - val_accuracy: 0.9785\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.9793 - val_loss: 0.0809 - val_accuracy: 0.9774\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9809 - val_loss: 0.0765 - val_accuracy: 0.9778\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9820 - val_loss: 0.0790 - val_accuracy: 0.9778\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9819 - val_loss: 0.0820 - val_accuracy: 0.9782\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9825 - val_loss: 0.0787 - val_accuracy: 0.9793\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0524 - accuracy: 0.9828 - val_loss: 0.0831 - val_accuracy: 0.9782\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0489 - accuracy: 0.9836 - val_loss: 0.0841 - val_accuracy: 0.9793\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9835 - val_loss: 0.0781 - val_accuracy: 0.9798\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0490 - accuracy: 0.9844 - val_loss: 0.0800 - val_accuracy: 0.9787\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0465 - accuracy: 0.9839 - val_loss: 0.0757 - val_accuracy: 0.9805\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0447 - accuracy: 0.9850 - val_loss: 0.0774 - val_accuracy: 0.9798\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0444 - accuracy: 0.9850 - val_loss: 0.0843 - val_accuracy: 0.9803\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0466 - accuracy: 0.9848 - val_loss: 0.0814 - val_accuracy: 0.9801\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9858 - val_loss: 0.0793 - val_accuracy: 0.9797\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0429 - accuracy: 0.9855 - val_loss: 0.0763 - val_accuracy: 0.9803\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9861 - val_loss: 0.0801 - val_accuracy: 0.9795\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.9872 - val_loss: 0.0819 - val_accuracy: 0.9796\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9859 - val_loss: 0.0842 - val_accuracy: 0.9798\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0402 - accuracy: 0.9867 - val_loss: 0.0820 - val_accuracy: 0.9811\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0384 - accuracy: 0.9871 - val_loss: 0.0819 - val_accuracy: 0.9809\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0399 - accuracy: 0.9872 - val_loss: 0.0816 - val_accuracy: 0.9804\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.9870 - val_loss: 0.0823 - val_accuracy: 0.9820\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0382 - accuracy: 0.9873 - val_loss: 0.0826 - val_accuracy: 0.9809\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.9884 - val_loss: 0.0812 - val_accuracy: 0.9814\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0370 - accuracy: 0.9875 - val_loss: 0.0828 - val_accuracy: 0.9800\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9887 - val_loss: 0.0852 - val_accuracy: 0.9799\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.9875 - val_loss: 0.0883 - val_accuracy: 0.9803\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0359 - accuracy: 0.9881 - val_loss: 0.0849 - val_accuracy: 0.9809\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0322 - accuracy: 0.9896 - val_loss: 0.0900 - val_accuracy: 0.9800\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0379 - accuracy: 0.9876 - val_loss: 0.0848 - val_accuracy: 0.9798\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9887 - val_loss: 0.0861 - val_accuracy: 0.9804\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.9888 - val_loss: 0.0842 - val_accuracy: 0.9810\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 0.9898 - val_loss: 0.0892 - val_accuracy: 0.9815\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9886 - val_loss: 0.0872 - val_accuracy: 0.9791\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0316 - accuracy: 0.9897 - val_loss: 0.0865 - val_accuracy: 0.9808\n",
      "313/313 [==============================] - 0s 480us/step - loss: 0.0856 - accuracy: 0.9804\n",
      "Test accuracy: 0.980400025844574\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "#for tensorboard\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard_callback = TensorBoard('.logdir')\n",
    "\n",
    "# network and training\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# loading MNIST dataset\n",
    "# verify\n",
    "# the split between train and test is 60,000, and 10,000 respectly \n",
    "# one-hot is automatically applied\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#normalize in [0,1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "#one-hot\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "#build the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "   \t\tinput_shape=(RESHAPED,),\n",
    "   \t\tname='dense_layer', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "   \t\tname='dense_layer_2', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "   \t\tname='dense_layer_3', activation='softmax'))\n",
    "\n",
    "# summary of the model\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer='Adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# # callbacks\n",
    "\n",
    "# callbacks=[\n",
    "#     tf.keras.callbacks.TensorBoard(log_dir='./logs/example')\n",
    "# ]\n",
    "\n",
    "\n",
    "#training the moodel\n",
    "model.fit(X_train, Y_train,\n",
    "\t\tbatch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "\t\tverbose=VERBOSE, validation_split=VALIDATION_SPLIT,\n",
    "\t\tcallbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "#evalute the model\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# making prediction\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 개선 방법 \n",
    "\n",
    "- 가중치 초기화 설정, 배치정규화 등\n",
    "- hyperparameter 조정하기\n",
    "    - 학습률\n",
    "    - 은닉층의 수, 뉴런 수\n",
    "    - batch size\n",
    "    - ephoc 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-659355af52fe>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-659355af52fe>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --logdir= /logs/example/\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir= /logs/example/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
