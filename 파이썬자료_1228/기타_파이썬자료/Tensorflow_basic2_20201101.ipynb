{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tensorflow 1.x 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/byunghwalim/tfenv/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "[3 3 8 7]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "v_1=tf.constant([1,2,3,4])\n",
    "v_2=tf.constant([2,1,5,3])\n",
    "v_add=tf.add(v_1,v_2)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(v_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 8 7]\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "print(sess.run(v_add))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_4:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Const_5:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"zeros_1:0\", shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## 연산의 예시\n",
    "tfs=tf.InteractiveSession()\n",
    "\n",
    "t_1=tf.constant(4)\n",
    "t_2=tf.constant([4,3,2])\n",
    "zero_t=tf.zeros([2,3], tf.int32)\n",
    "\n",
    "print(t_1)\n",
    "print(t_2)\n",
    "print(zero_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_like_1:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"ones_like_1:0\", shape=(3,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.zeros_like(t_2))\n",
    "print(tf.ones_like(t_2))\n",
    "tfs.run(tf.zeros_like(t_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul_6:0\", shape=(3, 3), dtype=float32)\n",
      "Tensor(\"pow_3:0\", shape=(3, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  2.,  4.],\n",
       "       [ 6.,  8., 10.],\n",
       "       [12., 14., 16.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 넘파이와 유사하게 브로드캐스트 \n",
    "t=tf.Variable([[0.,1.,2.],[3.,4.,5.,],[6.,7.,8.]])\n",
    "\n",
    "print(t*2)\n",
    "print(t**2)\n",
    "\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "tfs.run(init)\n",
    "tfs.run(t*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"linspace_1/Slice:0\", shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# sequence\n",
    "\n",
    "range_t=tf.linspace(2.0, 5.0, 5)\n",
    "print(range_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"range:0\", shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "range_t2=tf.range(10)\n",
    "print(range_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable 변수의 정의\n",
    "\n",
    "rand_t=tf.random_uniform([50,50],0,10,seed=0)\n",
    "t_a=tf.Variable(rand_t)\n",
    "t_b=tf.Variable(rand_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=tf.Variable(tf.random_normal([100,100], stddev=2))\n",
    "bias=tf.Variable(tf.zeros([100], tf.float32), name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 초기화\n",
    "# 선언된 모든 변수는 명시적으로 값을 초기화해야 하므로 변수의 정의에서 초기화 방법을 지정해야 한다.\n",
    "# 계산 그래프의 정의에서 연산 객체를 선언해 초기화한다.\n",
    "\n",
    "intial_op=tf.global_variables_initializer()\n",
    "# 그래프를 실행하는 동안 tf.Variable.initializer를 사용하면 각 변수를 개별적으로 초기화 할 수 있다.\n",
    "bias=tf.Variable(tf.zeros([100,100]))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(bias.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 저장 : Saver 클래스를 사용해 모든 변수를 저장할 수 있다. \n",
    "\n",
    "saver=tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.508035   9.100512  14.233702  15.133608   4.6073093]\n",
      " [ 2.4462204 14.324924  19.133883  11.854273   2.2185783]\n",
      " [ 6.9019775  7.896036  18.042809  10.579325  17.508574 ]\n",
      " [14.804514  16.119246   6.4932747  5.0442753  7.771826 ]]\n"
     ]
    }
   ],
   "source": [
    "# Placeholder\n",
    "\n",
    "# x에 대한 플레이스홀더를 정의하고 임의의 4*5 행렬에 대해 feed_dict를 사용해 y=2x를 계산\n",
    "\n",
    "x=tf.placeholder(\"float\")\n",
    "y=2*x\n",
    "data=tf.random_uniform([4,5],10)\n",
    "with tf.Session() as sess:\n",
    "    x_data=sess.run(data)\n",
    "    print(sess.run(y, feed_dict={x:x_data}))   # feed_dict 는 텐서플로 플레이스홀더에 값을 공급하는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensorflow 2.x 이해\n",
    "\n",
    "- 텐서플로 1.x API는 심층 신경망과 다른 많은 유형의 머신러닝 프로그램을 나타내는 계산 그래프를 생성하고 조작하는 유연한 방법을 제공\n",
    "- 텐서플로 2.x.는 더 낮은 수준의 세부 정보를 추상화하는 높은 수준의 API를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Python\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 1.x 에서 계산그래프 보기\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "in_a=tf.placeholder(dtype=tf.float32, shape=(2))\n",
    "\n",
    "def model(x):\n",
    "    with tf.variable_scope(\"matmul\"):\n",
    "        W=tf.get_variable(\"W\", initializer=tf.ones(shape=(2,2)))\n",
    "        b=tf.get_variable(\"b\", initializer=tf.zeros(shape=(2)))\n",
    "        return x * W + b\n",
    "    \n",
    "out_a=model(in_a)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outs=sess.run([out_a],\n",
    "                 feed_dict={in_a:[1,0]})\n",
    "    writer=tf.summary.FileWriter(\"./logs/example/\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.3.0 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./logs/example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy처럼 텐서플로 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_16:0' shape=(2, 3) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy처럼 텐서플로 사용하기\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "t = tf.constant([[1.,2.,3.],[4.,5.,6.]]) # matrix\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_17:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_18:0' shape=(2, 3) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 크기와 데이터타입을 갖음\n",
    "\n",
    "t=tf.constant([[1.,2.,3.],[4.,5.,6.]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_7:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# index \n",
    "print(t[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_3:0' shape=(2, 3) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto graph\n",
    "\n",
    "- 텐서플로가 그래프를 생성하는 방법은 먼저 파이썬 함수의 코드를 분석하여 for문, while문, if문은 물론 break, continue, return과 같은 제어문을 모두 찾음\n",
    "- 이 단계를 오토그래프라고 함.\n",
    "- 텐서플로가 소스코드를 분석하는 이유는 파이썬이 제어문을 찾을 수 있느 방법을 제공하지 않기 때문\n",
    "- 함수의 코드를 분석한 후 오토그래프는 이 함수의 모든 제어문을 텐서플로 연산으로 바꾼 업그레이된 버전을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오토 그래프 (auto graph) : @tf.function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def linear_layer(x):\n",
    "  return 3 * x + 2\n",
    "\n",
    "@tf.function\n",
    "def simple_nn(x):\n",
    "  return tf.nn.relu(linear_layer(x))\n",
    "\n",
    "\n",
    "def simple_function(x):\n",
    "\treturn 3*x\n",
    "\n",
    "# internal look at the autogenerated code\n",
    "print(tf.autograph.to_code(simple_nn.python_function, experimental_optional_features=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Keras API\n",
    "\n",
    "### 1. Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data.datasets로 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5ef98d34c8c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.0.1-py3-none-any.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 518 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor in ./lib/python3.8/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: attrs>=18.1.0 in ./lib/python3.8/site-packages (from tensorflow-datasets) (20.2.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 86.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.24.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in ./lib/python3.8/site-packages (from tensorflow-datasets) (2.24.0)\n",
      "Requirement already satisfied: numpy in ./lib/python3.8/site-packages (from tensorflow-datasets) (1.18.5)\n",
      "Requirement already satisfied: six in ./lib/python3.8/site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 34.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.2.zip (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 49.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in ./lib/python3.8/site-packages (from tensorflow-datasets) (3.13.0)\n",
      "Requirement already satisfied: absl-py in ./lib/python3.8/site-packages (from tensorflow-datasets) (0.10.0)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.5-cp38-cp38-macosx_10_14_x86_64.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 37.6 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in ./lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.8/site-packages (from protobuf>=3.6.1->tensorflow-datasets) (49.2.1)\n",
      "Building wheels for collected packages: promise, future, dill\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=45455bf89d57ed4613ed18a94b648c988f8984fc2afda1172dfdfa4757f2bf9c\n",
      "  Stored in directory: /Users/byunghwalim/Library/Caches/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=c42732697f80b344ba7ec59792339239617460c07259c41f54a752c1f734a85a\n",
      "  Stored in directory: /Users/byunghwalim/Library/Caches/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78912 sha256=f9630882b3772e3c9c9f1cd49da2f706efe42d2bee9d73e4c4c7712a542520c9\n",
      "  Stored in directory: /Users/byunghwalim/Library/Caches/pip/wheels/93/7f/7d/78ec535a4340ef2696aad8b17fe8bb063d56301bd62881b069\n",
      "Successfully built promise future dill\n",
      "Installing collected packages: promise, future, googleapis-common-protos, tensorflow-metadata, tqdm, dill, importlib-resources, dm-tree, tensorflow-datasets\n",
      "Successfully installed dill-0.3.2 dm-tree-0.1.5 future-0.18.2 googleapis-common-protos-1.52.0 importlib-resources-3.3.0 promise-2.3 tensorflow-datasets-4.0.1 tensorflow-metadata-0.24.0 tqdm-4.51.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/byunghwalim/tfenv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset', 'ai2_arc', 'ai2_arc_with_ir', 'amazon_us_reviews', 'anli', 'arc', 'bair_robot_pushing_small', 'bccd', 'beans', 'big_patent', 'bigearthnet', 'billsum', 'binarized_mnist', 'binary_alpha_digits', 'blimp', 'bool_q', 'c4', 'caltech101', 'caltech_birds2010', 'caltech_birds2011', 'cars196', 'cassava', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'cfq', 'chexpert', 'cifar10', 'cifar100', 'cifar10_1', 'cifar10_corrupted', 'citrus_leaves', 'cityscapes', 'civil_comments', 'clevr', 'clic', 'clinc_oos', 'cmaterdb', 'cnn_dailymail', 'coco', 'coco_captions', 'coil100', 'colorectal_histology', 'colorectal_histology_large', 'common_voice', 'coqa', 'cos_e', 'cosmos_qa', 'covid19sum', 'crema_d', 'curated_breast_imaging_ddsm', 'cycle_gan', 'deep_weeds', 'definite_pronoun_resolution', 'dementiabank', 'diabetic_retinopathy_detection', 'div2k', 'dmlab', 'downsampled_imagenet', 'dsprites', 'dtd', 'duke_ultrasound', 'emnist', 'eraser_multi_rc', 'esnli', 'eurosat', 'fashion_mnist', 'flic', 'flores', 'food101', 'forest_fires', 'fuss', 'gap', 'geirhos_conflict_stimuli', 'genomics_ood', 'german_credit_numeric', 'gigaword', 'glue', 'goemotions', 'gpt3', 'groove', 'gtzan', 'gtzan_music_speech', 'hellaswag', 'higgs', 'horses_or_humans', 'i_naturalist2017', 'imagenet2012', 'imagenet2012_corrupted', 'imagenet2012_real', 'imagenet2012_subset', 'imagenet_a', 'imagenet_r', 'imagenet_resized', 'imagenet_v2', 'imagenette', 'imagewang', 'imdb_reviews', 'irc_disentanglement', 'iris', 'kitti', 'kmnist', 'lfw', 'librispeech', 'librispeech_lm', 'libritts', 'ljspeech', 'lm1b', 'lost_and_found', 'lsun', 'malaria', 'math_dataset', 'mctaco', 'mnist', 'mnist_corrupted', 'movie_lens', 'movie_rationales', 'movielens', 'moving_mnist', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'natural_questions', 'natural_questions_open', 'newsroom', 'nsynth', 'nyu_depth_v2', 'omniglot', 'open_images_challenge2019_detection', 'open_images_v4', 'openbookqa', 'opinion_abstracts', 'opinosis', 'opus', 'oxford_flowers102', 'oxford_iiit_pet', 'para_crawl', 'patch_camelyon', 'paws_wiki', 'paws_x_wiki', 'pet_finder', 'pg19', 'places365_small', 'plant_leaves', 'plant_village', 'plantae_k', 'qa4mre', 'qasc', 'quickdraw_bitmap', 'radon', 'reddit', 'reddit_disentanglement', 'reddit_tifu', 'resisc45', 'robonet', 'rock_paper_scissors', 'rock_you', 'salient_span_wikipedia', 'samsum', 'savee', 'scan', 'scene_parse150', 'scicite', 'scientific_papers', 'sentiment140', 'shapes3d', 'smallnorb', 'snli', 'so2sat', 'speech_commands', 'spoken_digit', 'squad', 'stanford_dogs', 'stanford_online_products', 'starcraft_video', 'stl10', 'sun397', 'super_glue', 'svhn_cropped', 'ted_hrlr_translate', 'ted_multi_translate', 'tedlium', 'tf_flowers', 'the300w_lp', 'tiny_shakespeare', 'titanic', 'trec', 'trivia_qa', 'tydi_qa', 'uc_merced', 'ucf101', 'vctk', 'vgg_face2', 'visual_domain_decathlon', 'voc', 'voxceleb', 'voxforge', 'waymo_open_dataset', 'web_questions', 'wider_face', 'wiki40b', 'wikihow', 'wikipedia', 'wikipedia_toxicity_subtypes', 'wine_quality', 'winogrande', 'wmt14_translate', 'wmt15_translate', 'wmt16_translate', 'wmt17_translate', 'wmt18_translate', 'wmt19_translate', 'wmt_t2t_translate', 'wmt_translate', 'wordnet', 'xnli', 'xquad', 'xsum', 'yelp_polarity_reviews', 'yes_no']\n"
     ]
    }
   ],
   "source": [
    "builders=tfds.list_builders()\n",
    "print(builders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    version=3.0.1,\n",
      "    description='The MNIST database of handwritten digits.',\n",
      "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    total_num_examples=70000,\n",
      "    splits={\n",
      "        'test': 10000,\n",
      "        'train': 60000,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#name에 지정된 데이터셋을 DatasetInfo 메타데이터와 함께 로드\n",
    "data, info=tfds.load(\"mnist\", with_info=True, try_gcs=True)\n",
    "train_data, test_data=data['train'], data['test']\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 tf.keras와 추정기\n",
    "\n",
    "\n",
    "### 각 은닉층에 10개의 노드가 있으며, 2개의 은닉층을 가진 DNN 구축\n",
    "\n",
    "```\n",
    "classifier = tf.estimator.DNNClassifier(feature_columns=my_feature_columns, # 모델에서 사용하려는 단일 특징을 설명하는 열의 목록\n",
    "                                       # 각 10개의 노드를 가진 두 은닉층\n",
    "                                       hidden_units=[10,10],\n",
    "                                       # 모델은 3개의 부류 중에서 선택\n",
    "                                       n_classes=3)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "<BatchDataset shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "# MNIST 로드, 크기 변환, 섞기, 배치\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def input_fn(mode):\n",
    "  datasets, info = tfds.load(name='mnist',\n",
    "                                with_info=True,\n",
    "                                as_supervised=True, try_gcs=True)\n",
    "  mnist_dataset = (datasets['train'] if mode == tf.estimator.ModeKeys.TRAIN else\n",
    "                   datasets['test'])\n",
    "\n",
    "  def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "  return mnist_dataset.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test = input_fn('test')\n",
    "train = input_fn(tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "print(test)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-41776d0793d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m tf.estimator.train_and_evaluate(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0meval_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# 추정기는 tf.estimator.train_and_evaluation()를 사용하며, 데이터에 반복 작업하는 input_fn을 전달\n",
    "\n",
    "tf.estimator.train_and_evaluate(\n",
    "    classifier,\n",
    "    train_spec=tf.estimator.TrainSpec(input_fn=input_fn),\n",
    "    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"AddN_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"AddN_2:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(4.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 4)\n",
    "dy_dx = g.gradient(y, x)  # 6.0\n",
    "print (dz_dx)\n",
    "print (dy_dx)\n",
    "del g  # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
