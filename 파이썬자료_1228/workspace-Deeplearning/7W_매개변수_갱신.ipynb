{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7W \n",
    "\n",
    "# 가중치 업데이트\n",
    "\n",
    "    - 모멘텀 ( 스텝 방향 ) \n",
    "    - RMSpro ( 스텝 사이즈 ) \n",
    "    - Adam  ( 스텝 방향 + 스텝사이즈 ) \n",
    "    \n",
    "    \n",
    "# 가중치 설정 \n",
    "\n",
    "\n",
    "    - Xavier\n",
    "    - He\n",
    "    \n",
    "# 배치 정규화\n",
    "\n",
    "\n",
    "# 드랍아웃\n",
    "\n",
    "# Others\n",
    "\n",
    "    - L2 정규화\n",
    "    - 옵티마이징 하이퍼파라미터 (외생변수)\n",
    "        1. 히든레이어는 몇개? , 노드는 몇개? , 학습률 , 등등 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "sn.set()\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "fl = fm.FontProperties(fname =\"C:\\Windows\\Fonts\\malgun.ttf\").get_name()\n",
    "plt.rc('font',family=fl)\n",
    "\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from mnist import load_mnist\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__ (self, lr = 0.01):\n",
    "        self.lr = lr \n",
    "        \n",
    "    def update ( self , params , grads ):\n",
    "        for key in params.key():\n",
    "            params[key] -= lr*grads[key] # W  = W - n * dw \n",
    "            \n",
    "#################################################################################################           \n",
    "            \n",
    "class Momentum : \n",
    "    \n",
    "    def __init__ ( self, lr = 0.01 , momentum = 0.9 ):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None \n",
    "        \n",
    "    def update ( self , params , grads):\n",
    "        \n",
    "        if self.v in None:   # 만약 v 가 없다면 0으로 채우라 \n",
    "            self.v = {}\n",
    "            \n",
    "            for key , val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key , val in params.items():\n",
    "                self.v[key] = self.momentum* self.v[key] - self.lr * grads[key] # W = alpha * V  - n * dw \n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "#################################################################################################                  \n",
    "                \n",
    "class AdaGard : \n",
    "    \n",
    "    def __init__ (self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None # h 생성 \n",
    "        \n",
    "    def update ( self, params , grads ):\n",
    "        \n",
    "        if self.h is None :  # h가 만약 없다면 0 으로 채우기 \n",
    "            self.h = {}\n",
    "            \n",
    "            for key , val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self. lr* grads[key] / (np.sqrt ( self.h[key] ) + 1e-7)  ### 업데이트 구간 \n",
    "            \n",
    "#################################################################################################  \n",
    "            \n",
    "class RMSprop : \n",
    "    \n",
    "    def __init__ (self, lr = 0.01  , decay_rate = 0.99 ):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate # beta \n",
    "        self.h = None # h 생성 \n",
    "        \n",
    "    def update ( self, params , grads ):\n",
    "        \n",
    "        if self.h is None :  # h가 만약 없다면 0 으로 채우기 \n",
    "            self.h = {}\n",
    "            \n",
    "            for key , val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            \n",
    "            self.h[key] *= self.decay_rate \n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            \n",
    "            params[key] -= self. lr* grads[key] / (np.sqrt ( self.h[key] ) + 1e-7)  ### 업데이트 구간  \n",
    "\n",
    "            \n",
    "#################################################################################################  \n",
    "\n",
    "class Adam : \n",
    "    \n",
    "    def __init__( self , lr = 0.001 , beta1= 0.9 , beta2 = 0.999):\n",
    "        self.lr = lr \n",
    "        self.beta1 = beta1 #  좌우 이동평균  # v = beta 1 + ( 1- beta1) * dw \n",
    "        self.beta2 = beta2 #  상하 이동평균  # w = beat2 + ( 1 - beat2 ) * dw**2\n",
    "        self.iter = 0 \n",
    "        self.m = None # 좌우 빠르게   # 스텝방향  # 모멘텀 # V\n",
    "        self.v = None # 상하는 느리게 # 스텝사이즈 # RMSprop  # H\n",
    "        \n",
    "    def update ( self , params , grads):\n",
    "        \n",
    "        if self.m in None :\n",
    "            self.m , self. v = {} , {}\n",
    "            \n",
    "            for key , val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            self.iter += 1 \n",
    "            # 업데이트 평균을 보정하기 위한 값 ( 초반엔 평균값이 없기 때문에 )\n",
    "            # sqrt(1/h)\n",
    "            lr_t = self.lr * np.sqrt( 1.0 - self.beta2 ** self.iter)/ (1.0 - self.beta1 **self.iter) # eta 대신 sqrt( 1/h )\n",
    "            \n",
    "        for key in params.key():\n",
    "            \n",
    "            # V = 1 - beta 1 * dw - Vdw \n",
    "            self.m[key] += ( 1- self.beta1) * ( grads[key] - self.m[key]) # 모멘텀 부분  # dw 에 대한 이동평균 \n",
    "            # h = 1 - beta2  * dw - Sdw \n",
    "            self.v[key] += ( 1- self.beta2) * ( grads[key] **2 -  self.v[key]) #  RMSprom 부분 # dw**2 에 대한 이동평균 \n",
    "            \n",
    "            # W = W - eta * 모멘텀 / sqrt( RMSprop ) +  보정값 \n",
    "            params[key] -= lr_t * self.m[key] / ( np.sqrt(self.v[key])  + 1e-7 ) # 업데이트 부분 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심층신경망 모형 ( 신경망 구조 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers import *\n",
    "\n",
    "from collections import OrderedDict  \n",
    "# from collections import OrderedDict\n",
    "# key 와 value 에서 key 값의 순서를 자동지정함으로써 Affine 계층 ( ex 1,2,3...), Relu 계층의 순서를 명확하게 함.\n",
    "\n",
    "class MultiLayerNet:\n",
    "\n",
    "    \"\"\" 완전연결 다층 신경망 ( fully - connected DNN 모든구성을 다 가진 신경망 )\n",
    "    \n",
    "        input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        '기본설정' 은 weight_init_std = 0.01\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ ( self , input_size , hidden_size_list , output_size , activation ='relu' , weight_init_std='relu',\n",
    "                 weight_decay_lambda = 0):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_size_layer_num = len(hidden_size_list)\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.params = {} \n",
    "        \n",
    "# 가중치 초기화 \n",
    "        self.__init_weight(weight_init_std)\n",
    "        \n",
    "# 신경망 계층 생성 \n",
    "        \n",
    "        activation_layer = { 'Sigmoid' : sigmoid , 'relu' : Relu }\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        for idx in range(1, self.hidden_size_layer_num +1): \n",
    "                            # 1로시작해서 ~ 전체 히든 레이어 까지 반복하라 \n",
    "            \n",
    "            self.layers['Affine' + str(idx)] = Affine( self.params[ 'W' + str(idx)], self.params['b'+ str(idx)])\n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "\n",
    "        idx = self.hidden_layer_num + 1 # idx 도 전체 히든 레이어 까지 반복 \n",
    "        \n",
    "# 마지막 소프트맥스 이전 레이어 \n",
    "        \n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "# 소프트 맥스 실행 , 소프트맥스 레이어는 self.last_layer\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    \n",
    "    def __init_weight(self, weight_init_std):\n",
    "        \"\"\"가중치 초기화\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "        \"\"\"\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLU를 사용할 때의 권장 초깃값\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoid를 사용할 때의 권장 초깃값\n",
    "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 비용함수 , 손실함수 \n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블 , 타겟 벨류  \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        손실 함수의 값\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        weight_decay = 0\n",
    "        \n",
    "# L2 정규화 할 때 사용 , 없어도 됨.  \n",
    "        \n",
    "        for idx in range(1, self.hidden_layer_num + 2): \n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
    "\n",
    "        return self.last_layer.forward(y, t) + weight_decay\n",
    "\n",
    "    \n",
    "# 출력 \n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        \n",
    "# 만약에 1이 아니라면 제일 큰 위치로 계산 \n",
    "\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "# y 가 t 값에 같다면 평균값 출력 \n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(수치 미분).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "# 결과 저장 \n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "# 순전파 \n",
    "        self.loss(x, t)\n",
    "\n",
    "# 역전파\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "# 레이어의 값들을 reverse()  하여 \n",
    "# dout 을 역전파 하라 \n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "# 결과 저장  ( dw 리턴 )\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "\n",
    "# L2 정규화를 한다면 + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W 부분이 필요함 \n",
    "\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W\n",
    "        \n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer 성능비교 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "    하이퍼파라미터  탐색은 최적값이 존재할 법한 범위를 점차 좁히면서 하는것이 효과적이다. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
