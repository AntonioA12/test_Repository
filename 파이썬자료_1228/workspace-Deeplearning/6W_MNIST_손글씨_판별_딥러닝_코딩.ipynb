{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "sn.set()\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "fl = fm.FontProperties(fname =\"C:\\Windows\\Fonts\\malgun.ttf\").get_name()\n",
    "plt.rc('font',family=fl)\n",
    "\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from mnist import load_mnist\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - 손글씨 판별 딥러닝 코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2: # x 크기는 2 \n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # softmax 값이 너무 클때를 방지 # 오버플로우 \n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error( y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    if t.size == y.size : # t.szie 와 y.size 는 값이 같아야 함\n",
    "        t = t.argmax(axis=1)\n",
    "            \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log ( y[ np.arange(batch_size) , t ]+1e-7 ) )/ batch_size\n",
    "    \n",
    "##############################################################################################            \n",
    "        \n",
    "        \n",
    "class Relu: \n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x): # x 는 인풋값 \n",
    "        self.mask = (x<=0) # 0 보다 작으면 \n",
    "        out = x.copy() # x를 그대로 아웃\n",
    "        out[self.mask] = 0 # 0 보다 작으면 0 으로 아웃\n",
    "            \n",
    "    def backward( self , dout ): # dout 은 미분의 곱, 미분의 토탈 \n",
    "        dout[self.mask] = 0 \n",
    "        dx = dout\n",
    "        return dx \n",
    "    \n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "    \n",
    "##############################################################################################    \n",
    "    \n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None \n",
    "        \n",
    "    def forward(self , x ): # x 를 시그모이드 적용후 바로 아웃 \n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out \n",
    "    \n",
    "    def backward( self , dout ): # dout 은 z->y 에서의 y 이다.\n",
    "        dx = dout * ( 1.0 - self.out) * self.out\n",
    "        return dx \n",
    "    \n",
    "##############################################################################################    \n",
    "    \n",
    "class Affine:\n",
    "    def __init__(self , W , b ): # Y = np.dot(X,W)+B 부분, 입력값 과 weight 값 두개 가 필요 \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None # transpose 안한 x shape\n",
    "        \n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    \n",
    "    def forward(self , x):\n",
    "        self.original_x_shape = x.shape  # x.shape [ 행 , 열 ]\n",
    "        x = x.reshape(x.shape[0], -1) # x[행, 열]\n",
    "        self.x = x\n",
    "        out = np.dot(self.x , self.W)+self.b\n",
    "        \n",
    "        return out \n",
    "        \n",
    "    def backward(self , dout):\n",
    "        \n",
    "        dx = np.dot(dout , self.W.T)\n",
    "        \n",
    "        self.dw = np.dot(self.x.T , dout )\n",
    "        \n",
    "        self.db = np.sum( dout , axis = 0)\n",
    "        \n",
    "        dx =  dx.reshape(*self.original_x_shape)\n",
    "        \n",
    "        return dx\n",
    "        \n",
    "##############################################################################################            \n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__ (self):\n",
    "        self.loss = None\n",
    "        self.y = None #   y 의 순전파값\n",
    "        self.t = None # 0 또는 1,  실제값\n",
    "        \n",
    "    def forward( self , x ,t ):\n",
    "        self.t = t \n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error( self.y , self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backwaord ( self , dout=1):\n",
    "        batch_size = self.t.shape[0] \n",
    "        \n",
    "        if self.t.size == self.y.size :\n",
    "            dx = (self.y - self.x) / batch_size # 배치사이즈로 나누기 = 평균으로 나눔 \n",
    "            # dx 는 t 사이즈 - y 사이즈 서로간의 차이의 평균 즉, dx = 미분값 \n",
    "            \n",
    "        else: # y 의 값이 0 또는 1 둘다 아닐경우 ( 에러방지)\n",
    "            dx = self.y.copy()\n",
    "            dx[ np.arange(batch_size), self.t ] -= 1 \n",
    "            dx = dx /batch_size\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 구조 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict  \n",
    "# key 와 value 에서 key 값의 순서를 자동지정함으로써 Affine 계층 ( ex 1,2,3...), Relu 계층의 순서를 명확하게 함.\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__( self, input_size , hidden_size , output_size , weight_std = 0.01):\n",
    "        # 가중치 초기화 \n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1']=weight_std * np.random.randn(input_size , hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "        \n",
    "        # 계층 , layers\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1']=Affine(self.params['W1'] , self.params['b1'])\n",
    "        self.layers['Relu']=Relu()\n",
    "        self.layers['Affine2']=Affine(self.params['W2'] , self.params['b2'])\n",
    "        self.layers['Relu']=Relu()\n",
    "        \n",
    "        self.lastlayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self , x): # x가 들어올때, y 의 값은? 즉, SoftmaxWithloss 값은 무엇이냐 \n",
    "        \n",
    "        for layers in self.layers.values(): \n",
    "        # self.layers.values() = ['Affine1'] 과 ['Affine2']  의 결과값 을 이용해 매층마다 순전파로 Affine 계산 \n",
    "            x = layers.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastlayer.forward(y,t)\n",
    "    \n",
    "    def accuracy( self ,  x , t ): # predict() 에서 x 값으로 결과값 출력하기 , t 는 테스트 셋 \n",
    "        y = self.predict(x) \n",
    "        y = np.argmax(y, axis=1)\n",
    "        \n",
    "        if t.ndim !=1 : t = np.argmax(t, axis =1)\n",
    "            \n",
    "        \n",
    "        accuracy = np.sum ( y == t) / float(x.shape[0]) # y 와 t 가 같은 값은 몇개인가 \n",
    "        return accuracy\n",
    "    \n",
    "    # 오차역전파 \n",
    "    def gradient ( self , x , t):\n",
    "        # forward \n",
    "        self.loss(x,t)\n",
    "        \n",
    "        # backward\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout) # dout  은 편미분값 \n",
    "        \n",
    "        layers = list(self.layers.values()) # layers 는 Affine 이후 values 들 \n",
    "        layers.reverse() # 역행 \n",
    "        \n",
    "        for layer in layers : \n",
    "            dout = layer.backward(dout) # 계층만큼 편미분 누적화 \n",
    "            \n",
    "        grads={} \n",
    "        grads['W1'] , grads['b1'] = self.layers['Affine1'].dW , self.layers['Affine1'].db # Affine 의 dw , db \n",
    "        grads['W2'] , grads['b2'] = self.layers['Affine2'].dW , self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1087 0.1148\n",
      "0.9055 0.9093\n",
      "0.92345 0.9258\n",
      "0.9352833333333334 0.936\n",
      "0.9444833333333333 0.943\n",
      "0.9511833333333334 0.9502\n",
      "0.9563333333333334 0.9531\n",
      "0.9605333333333334 0.9571\n",
      "0.9647833333333333 0.9606\n",
      "0.9680666666666666 0.9637\n",
      "0.9700166666666666 0.965\n",
      "0.9713833333333334 0.9661\n",
      "0.9732166666666666 0.9658\n",
      "0.9756166666666667 0.9688\n",
      "0.9765166666666667 0.9693\n",
      "0.97795 0.9702\n",
      "0.9793833333333334 0.9696\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 실행 \n",
    "from layers import *\n",
    "from mnist import load_mnist\n",
    "\n",
    "(x_train , t_train) , ( x_test , t_test) = load_mnist(normalize=True , one_hot_label=True)\n",
    "\n",
    "network =TwoLayerNet( input_size= 784 , hidden_size=50 , output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 \n",
    "learning_rate = 0.1 \n",
    "\n",
    "train_loss_list=[] # loss 값 넣을 준비\n",
    "train_acc_list=[] # 트레이닝 값  넣을 준비 \n",
    "test_acc_list=[] # 테스트 값 넣을 준비 \n",
    "\n",
    "iter_per_ephoch = max(train_size/ batch_size,1)\n",
    "\n",
    "\n",
    "##############################################################################################    \n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size , batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파 \n",
    "    \n",
    "    grad = network.gradient( x_batch , t_batch )\n",
    "    \n",
    "    # 업데이트 \n",
    "    \n",
    "##############################################################################################    \n",
    "    \n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key] -=learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_ephoch == 0 :\n",
    "        train_acc = network.accuracy(x_train , t_train)\n",
    "        test_acc = network.accuracy( x_test , t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print( train_acc , test_acc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
