{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "sn.set()\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "fl = fm.FontProperties(fname =\"C:\\Windows\\Fonts\\malgun.ttf\").get_name()\n",
    "plt.rc('font',family=fl)\n",
    "\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from mnist import load_mnist\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - 손글씨 판별 딥러닝 코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2: # x 크기는 2 \n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # softmax 값이 너무 클때를 방지 # 오버플로우 \n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error( y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    if t.size == y.size : # t.szie 와 y.size 는 값이 같아야 함\n",
    "        t = t.armax(axis=1)\n",
    "            \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log ( y[ np.arrange(batch_size) , t ]+1e-7 ) )/ batch_size\n",
    "    \n",
    "##############################################################################################            \n",
    "        \n",
    "        \n",
    "class Relu: \n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x): # x 는 인풋값 \n",
    "        self.mask = (x<=0) # 0 보다 작으면 \n",
    "        out = x.copy() # x를 그대로 아웃\n",
    "        out[self.mask] = 0 # 0 보다 작으면 0 으로 아웃\n",
    "            \n",
    "    def backward( self , dout ): # dout 은 미분의 곱, 미분의 토탈 \n",
    "        dout[self.mask] = 0 \n",
    "        dx = dout\n",
    "        return dx \n",
    "    \n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "    \n",
    "##############################################################################################    \n",
    "    \n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None \n",
    "        \n",
    "    def forward(self , x ): # x 를 시그모이드 적용후 바로 아웃 \n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out \n",
    "    \n",
    "    def backward( self , dout ): # dout 은 z->y 에서의 y 이다.\n",
    "        dx = dout * ( 1.0 - self.out) * self.out\n",
    "        return dx \n",
    "    \n",
    "##############################################################################################    \n",
    "    \n",
    "class Affine:\n",
    "    def __init__(self , W , b ): # Y = np.dot(X,W)+B 부분, 입력값 과 weight 값 두개 가 필요 \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None # transpose 안한 x shape\n",
    "        \n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    \n",
    "    def forward(self , x):\n",
    "        self.original_x_shape = x.shape  # x.shape [ 행 , 열 ]\n",
    "        x = x.reshape(x.shape[0], -1) # x[행, 열]\n",
    "        self.x = x\n",
    "        out = np.dot(self.x , self.W)+self.b\n",
    "        \n",
    "        return out \n",
    "        \n",
    "    def backward(self , dout):\n",
    "        \n",
    "        dx = np.dot(dout , self.W.T)\n",
    "        \n",
    "        self.dw = np.dot(self.x.T , dout )\n",
    "        \n",
    "        self.db = np.sum( dout , axis = 0)\n",
    "        \n",
    "        dx =  dx.reshape(*self.original_x_shape)\n",
    "        \n",
    "        return dx\n",
    "        \n",
    "##############################################################################################            \n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__ (self):\n",
    "        self.loss = None\n",
    "        self.y = None #   y 의 순전파값\n",
    "        self.t = None # 0 또는 1,  실제값\n",
    "        \n",
    "    def forward( self , x ,t ):\n",
    "        self.t = t \n",
    "        self.y = softmax(y)\n",
    "        self.loss = cross_entropy_error( self.y , self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backwaord ( self , dout=1):\n",
    "        batch_size = self.t.shape[0] \n",
    "        \n",
    "        if self.t.size == self.y.size :\n",
    "            dx = (self.y - self.x) / batch_size # 배치사이즈로 나누기 = 평균으로 나눔 \n",
    "            # dx 는 t 사이즈 - y 사이즈 서로간의 차이의 평균 즉, dx = 미분값 \n",
    "            \n",
    "        else: # y 의 값이 0 또는 1 둘다 아닐경우 ( 에러방지)\n",
    "            dx = self.y.copy()\n",
    "            dx[ np.arrange(batch_size), self.t ] -= 1 \n",
    "            dx = dx /batch_size\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 구조 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict  \n",
    "# key 와 value 에서 key 값의 순서를 자동지정함으로써 Affine 계층 ( ex 1,2,3...), Relu 계층의 순서를 명확하게 함.\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__( self, input_size , hidden_size , output_size , weight_std = 0.01):\n",
    "        # 가중치 초기화 \n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1']=weight_std * np.random.randn(input_size , hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "        \n",
    "        # 계층 , layers\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1']=Affine(self.params['W1'] , self.params['b1'])\n",
    "        self.layers['Relu']=Relu()\n",
    "        self.layers['Affine2']=Affine(self.params['W2'] , self.params['b2'])\n",
    "        self.layers['Relu']=Relu()\n",
    "        \n",
    "        self.lastlayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self , x): # x가 들어올때, y 의 값은? 즉, SoftmaxWithloss 값은 무엇이냐 \n",
    "        \n",
    "        for layers in self.layers.values(): \n",
    "        # self.layers.values() = ['Affine1'] 과 ['Affine2']  의 결과값 을 이용해 매층마다 순전파로 Affine 계산 \n",
    "            x = layers.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastlayer.forward(y,t)\n",
    "    \n",
    "    def accuracy( self ,  x , t ): # predict() 에서 x 값으로 결과값 출력하기 , t 는 테스트 셋 \n",
    "        y = self.predict(x) \n",
    "        y = np.argmax(y, axis=1)\n",
    "        \n",
    "        if t.ndim !=1 : t = np.argmax(t, axis =1)\n",
    "            \n",
    "        \n",
    "        accuracy = np.sum ( y == t) / float(x.shape[0]) # y 와 t 가 같은 값은 몇개인가 \n",
    "        return accuracy\n",
    "    \n",
    "    # 오차역전파 \n",
    "    def gradient ( self , x , t):\n",
    "        # forward \n",
    "        self.loss(x,t)\n",
    "        \n",
    "        # backward\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout) # dout  은 편미분값 \n",
    "        \n",
    "        layers = list(self.layers.values()) # layers 는 Affine 이후 values 들 \n",
    "        layers.reverse() # 역행 \n",
    "        \n",
    "        for layer in layers : \n",
    "            dout = layer.backward(dout) # 계층만큼 편미분 누적화 \n",
    "            \n",
    "        grads={} \n",
    "        grads['W1'] , grads['b1'] = self.layers['Affine1'].dW , self.layers['Affine1'].db # Affine 의 dw , db \n",
    "        grads['W2'] , grads['b2'] = self.layers['Affine2'].dW , self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d72fe102523d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# 오차역전파\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mx_batch\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# 업데이트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-fcfe72c4c327>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-fcfe72c4c327>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlastlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[0mx\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# predict() 에서 x 값으로 결과값 출력하기 , t 는 테스트 셋\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-ee8ccbb856d8>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# 학습 및 실행 \n",
    "from mnist import load_mnist\n",
    "\n",
    "(x_train , t_train) , ( x_test , t_test) = load_mnist(normalize=True , one_hot_label=True)\n",
    "\n",
    "network =TwoLayerNet( input_size= 784 , hidden_size=50 , output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 \n",
    "learning_rate = 0.1 \n",
    "\n",
    "train_loss_list=[] # loss 값 넣을 준비\n",
    "train_acc_list=[] # 트레이닝 값  넣을 준비 \n",
    "test_acc_list=[] # 테스트 값 넣을 준비 \n",
    "\n",
    "iter_per_ephoch = max(train_size/ batch_size,1)\n",
    "\n",
    "\n",
    "##############################################################################################    \n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size , batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파 \n",
    "    \n",
    "    grad = network.gradient( x_batch , t_batch )\n",
    "    \n",
    "    # 업데이트 \n",
    "    \n",
    "##############################################################################################    \n",
    "    \n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key] -=learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_ephoch == 0 :\n",
    "        train_acc = network.accuracy(x_train , t_train)\n",
    "        test_acc = network.accuracy( x_test , t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print( train_acc , test_acc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
