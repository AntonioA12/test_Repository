{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "sn.set()\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "fl = fm.FontProperties(fname =\"C:\\Windows\\Fonts\\malgun.ttf\").get_name()\n",
    "plt.rc('font',family=fl)\n",
    "\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from mnist import load_mnist\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3069068122758165\n",
      "=== epoch:1, train acc:0.09333333333333334, test acc:0.1042 ===\n",
      "train loss:2.3145612645913114\n",
      "train loss:2.32547010870248\n",
      "train loss:2.328627561055833\n",
      "=== epoch:2, train acc:0.09333333333333334, test acc:0.1053 ===\n",
      "train loss:2.3188847658694045\n",
      "train loss:2.3265130574113506\n",
      "train loss:2.32324994345427\n",
      "=== epoch:3, train acc:0.09, test acc:0.1072 ===\n",
      "train loss:2.3232726877346477\n",
      "train loss:2.3045839058407656\n",
      "train loss:2.292609681411843\n",
      "=== epoch:4, train acc:0.09333333333333334, test acc:0.1085 ===\n",
      "train loss:2.3165451022930608\n",
      "train loss:2.2980231997594966\n",
      "train loss:2.3185250453134683\n",
      "=== epoch:5, train acc:0.09666666666666666, test acc:0.1089 ===\n",
      "train loss:2.3192871493748073\n",
      "train loss:2.3047540968249414\n",
      "train loss:2.2967404201868415\n",
      "=== epoch:6, train acc:0.1, test acc:0.1101 ===\n",
      "train loss:2.3167062635050315\n",
      "train loss:2.3077217047140888\n",
      "train loss:2.3131328169609264\n",
      "=== epoch:7, train acc:0.10333333333333333, test acc:0.1105 ===\n",
      "train loss:2.318088947122406\n",
      "train loss:2.287483636436701\n",
      "train loss:2.3152300369848646\n",
      "=== epoch:8, train acc:0.10333333333333333, test acc:0.1126 ===\n",
      "train loss:2.310235059118312\n",
      "train loss:2.3041371372302497\n",
      "train loss:2.3019902046012093\n",
      "=== epoch:9, train acc:0.10666666666666667, test acc:0.1126 ===\n",
      "train loss:2.300818967605419\n",
      "train loss:2.3063127985075518\n",
      "train loss:2.2896179114037767\n",
      "=== epoch:10, train acc:0.12333333333333334, test acc:0.1135 ===\n",
      "train loss:2.289502051966949\n",
      "train loss:2.3087423269262284\n",
      "train loss:2.2774194936096985\n",
      "=== epoch:11, train acc:0.12, test acc:0.114 ===\n",
      "train loss:2.2959833923529294\n",
      "train loss:2.297522577718511\n",
      "train loss:2.29540298101062\n",
      "=== epoch:12, train acc:0.13, test acc:0.1165 ===\n",
      "train loss:2.2993083733754487\n",
      "train loss:2.3101188696634183\n",
      "train loss:2.29176032689375\n",
      "=== epoch:13, train acc:0.13, test acc:0.1167 ===\n",
      "train loss:2.3004276136891453\n",
      "train loss:2.292850252349861\n",
      "train loss:2.284699749197668\n",
      "=== epoch:14, train acc:0.13, test acc:0.1172 ===\n",
      "train loss:2.2811506548570537\n",
      "train loss:2.288977229945102\n",
      "train loss:2.290784627678771\n",
      "=== epoch:15, train acc:0.13, test acc:0.1173 ===\n",
      "train loss:2.291052639437259\n",
      "train loss:2.290875049673219\n",
      "train loss:2.2858850929440973\n",
      "=== epoch:16, train acc:0.13666666666666666, test acc:0.1181 ===\n",
      "train loss:2.28227978614583\n",
      "train loss:2.2891285492775273\n",
      "train loss:2.2791905383102065\n",
      "=== epoch:17, train acc:0.13666666666666666, test acc:0.1199 ===\n",
      "train loss:2.285791724251346\n",
      "train loss:2.2875512366604904\n",
      "train loss:2.291661415455684\n",
      "=== epoch:18, train acc:0.13666666666666666, test acc:0.12 ===\n",
      "train loss:2.291782154624099\n",
      "train loss:2.286121259465881\n",
      "train loss:2.282262232928007\n",
      "=== epoch:19, train acc:0.14333333333333334, test acc:0.1193 ===\n",
      "train loss:2.2913557197205723\n",
      "train loss:2.303136036992586\n",
      "train loss:2.2914003136762258\n",
      "=== epoch:20, train acc:0.14666666666666667, test acc:0.1182 ===\n",
      "train loss:2.2876893186984164\n",
      "train loss:2.279037846508867\n",
      "train loss:2.281471021635278\n",
      "=== epoch:21, train acc:0.14, test acc:0.1193 ===\n",
      "train loss:2.2738330491527163\n",
      "train loss:2.283640048019098\n",
      "train loss:2.2748960088756744\n",
      "=== epoch:22, train acc:0.13666666666666666, test acc:0.1197 ===\n",
      "train loss:2.2785494137099236\n",
      "train loss:2.278049853059411\n",
      "train loss:2.277215233463348\n",
      "=== epoch:23, train acc:0.13666666666666666, test acc:0.1201 ===\n",
      "train loss:2.2596882861600194\n",
      "train loss:2.271588523930036\n",
      "train loss:2.279658285360367\n",
      "=== epoch:24, train acc:0.13666666666666666, test acc:0.1223 ===\n",
      "train loss:2.27449425099084\n",
      "train loss:2.276662170967496\n",
      "train loss:2.2757542384135783\n",
      "=== epoch:25, train acc:0.15, test acc:0.1258 ===\n",
      "train loss:2.2793740354517134\n",
      "train loss:2.279169746436228\n",
      "train loss:2.2729134118722016\n",
      "=== epoch:26, train acc:0.16333333333333333, test acc:0.1304 ===\n",
      "train loss:2.2804217076069313\n",
      "train loss:2.254853537517833\n",
      "train loss:2.25404392864866\n",
      "=== epoch:27, train acc:0.16333333333333333, test acc:0.1318 ===\n",
      "train loss:2.2707587731097885\n",
      "train loss:2.264979410660857\n",
      "train loss:2.2790691773010865\n",
      "=== epoch:28, train acc:0.16333333333333333, test acc:0.1345 ===\n",
      "train loss:2.275881771823353\n",
      "train loss:2.2677024201896843\n",
      "train loss:2.2685247855036526\n",
      "=== epoch:29, train acc:0.17333333333333334, test acc:0.1391 ===\n",
      "train loss:2.2824671758340007\n",
      "train loss:2.2725421754175117\n",
      "train loss:2.262750937494347\n",
      "=== epoch:30, train acc:0.18, test acc:0.1445 ===\n",
      "train loss:2.277822715272919\n",
      "train loss:2.272327228705898\n",
      "train loss:2.276494320723809\n",
      "=== epoch:31, train acc:0.18666666666666668, test acc:0.1509 ===\n",
      "train loss:2.254036126302812\n",
      "train loss:2.2622853238749\n",
      "train loss:2.269041933503001\n",
      "=== epoch:32, train acc:0.19, test acc:0.156 ===\n",
      "train loss:2.2558758985707605\n",
      "train loss:2.2677852448475972\n",
      "train loss:2.264077006829915\n",
      "=== epoch:33, train acc:0.19333333333333333, test acc:0.1606 ===\n",
      "train loss:2.2565659372083613\n",
      "train loss:2.257034505991875\n",
      "train loss:2.2719231628899426\n",
      "=== epoch:34, train acc:0.2, test acc:0.1632 ===\n",
      "train loss:2.2647539107270585\n",
      "train loss:2.267608651569188\n",
      "train loss:2.2694048384009684\n",
      "=== epoch:35, train acc:0.21, test acc:0.1696 ===\n",
      "train loss:2.2574402909187095\n",
      "train loss:2.26558905915486\n",
      "train loss:2.267839793929072\n",
      "=== epoch:36, train acc:0.22333333333333333, test acc:0.179 ===\n",
      "train loss:2.2556042078234553\n",
      "train loss:2.264196819527951\n",
      "train loss:2.2623759872297637\n",
      "=== epoch:37, train acc:0.22666666666666666, test acc:0.183 ===\n",
      "train loss:2.2637574140156875\n",
      "train loss:2.2696224259282407\n",
      "train loss:2.2564510322228415\n",
      "=== epoch:38, train acc:0.24333333333333335, test acc:0.1955 ===\n",
      "train loss:2.2477454370349252\n",
      "train loss:2.260341727761308\n",
      "train loss:2.2543964041916618\n",
      "=== epoch:39, train acc:0.25333333333333335, test acc:0.1997 ===\n",
      "train loss:2.2614954642153213\n",
      "train loss:2.258682177626136\n",
      "train loss:2.2564441288572694\n",
      "=== epoch:40, train acc:0.25666666666666665, test acc:0.2032 ===\n",
      "train loss:2.248355411593957\n",
      "train loss:2.2699244688181492\n",
      "train loss:2.270354557052308\n",
      "=== epoch:41, train acc:0.2633333333333333, test acc:0.2127 ===\n",
      "train loss:2.269957719024551\n",
      "train loss:2.2579332682095066\n",
      "train loss:2.2575966539287418\n",
      "=== epoch:42, train acc:0.31, test acc:0.2231 ===\n",
      "train loss:2.262079422602922\n",
      "train loss:2.249627019097676\n",
      "train loss:2.24319157622452\n",
      "=== epoch:43, train acc:0.31333333333333335, test acc:0.2233 ===\n",
      "train loss:2.242213347815901\n",
      "train loss:2.266865664251235\n",
      "train loss:2.2591820287609328\n",
      "=== epoch:44, train acc:0.31333333333333335, test acc:0.2309 ===\n",
      "train loss:2.253327182501238\n",
      "train loss:2.2479476465993837\n",
      "train loss:2.248684038028242\n",
      "=== epoch:45, train acc:0.31666666666666665, test acc:0.2358 ===\n",
      "train loss:2.2656350729320285\n",
      "train loss:2.2453801925366044\n",
      "train loss:2.2429895706280036\n",
      "=== epoch:46, train acc:0.31666666666666665, test acc:0.2398 ===\n",
      "train loss:2.2425221819167285\n",
      "train loss:2.2591566394669043\n",
      "train loss:2.2535518686634397\n",
      "=== epoch:47, train acc:0.32, test acc:0.2432 ===\n",
      "train loss:2.248179974893034\n",
      "train loss:2.262629003386488\n",
      "train loss:2.230404747257614\n",
      "=== epoch:48, train acc:0.33, test acc:0.2452 ===\n",
      "train loss:2.245592744751143\n",
      "train loss:2.2416568540685193\n",
      "train loss:2.2629717071281106\n",
      "=== epoch:49, train acc:0.33, test acc:0.2446 ===\n",
      "train loss:2.252255772570982\n",
      "train loss:2.2282859352751405\n",
      "train loss:2.2481817224893605\n",
      "=== epoch:50, train acc:0.3333333333333333, test acc:0.2488 ===\n",
      "train loss:2.2244752846566653\n",
      "train loss:2.2385387278068403\n",
      "train loss:2.235518447462867\n",
      "=== epoch:51, train acc:0.3433333333333333, test acc:0.2543 ===\n",
      "train loss:2.2364751021204383\n",
      "train loss:2.2510495507667634\n",
      "train loss:2.238498814025354\n",
      "=== epoch:52, train acc:0.35, test acc:0.2594 ===\n",
      "train loss:2.244529402016647\n",
      "train loss:2.255851049430467\n",
      "train loss:2.2548705452039917\n",
      "=== epoch:53, train acc:0.35, test acc:0.261 ===\n",
      "train loss:2.2482532528404\n",
      "train loss:2.2250444292313984\n",
      "train loss:2.237304726104377\n",
      "=== epoch:54, train acc:0.35, test acc:0.2637 ===\n",
      "train loss:2.2306297004295916\n",
      "train loss:2.232475381510743\n",
      "train loss:2.235557243784913\n",
      "=== epoch:55, train acc:0.35, test acc:0.2652 ===\n",
      "train loss:2.2340701220089327\n",
      "train loss:2.2326066907081334\n",
      "train loss:2.2340392075750324\n",
      "=== epoch:56, train acc:0.35, test acc:0.2689 ===\n",
      "train loss:2.236908660688353\n",
      "train loss:2.2424388299107783\n",
      "train loss:2.23104130454746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.35, test acc:0.2713 ===\n",
      "train loss:2.2372225432017956\n",
      "train loss:2.230405141350449\n",
      "train loss:2.241049954522445\n",
      "=== epoch:58, train acc:0.35, test acc:0.2731 ===\n",
      "train loss:2.2389368816716044\n",
      "train loss:2.2405683142630295\n",
      "train loss:2.241195819378794\n",
      "=== epoch:59, train acc:0.35333333333333333, test acc:0.2791 ===\n",
      "train loss:2.2325477009029084\n",
      "train loss:2.2277664211249815\n",
      "train loss:2.216073949375369\n",
      "=== epoch:60, train acc:0.35333333333333333, test acc:0.2808 ===\n",
      "train loss:2.240768724205946\n",
      "train loss:2.2411108000041575\n",
      "train loss:2.227885940940292\n",
      "=== epoch:61, train acc:0.35333333333333333, test acc:0.2833 ===\n",
      "train loss:2.2352635360048905\n",
      "train loss:2.2377910847081073\n",
      "train loss:2.2281469798618563\n",
      "=== epoch:62, train acc:0.36666666666666664, test acc:0.2858 ===\n",
      "train loss:2.253072260043042\n",
      "train loss:2.2163308147260796\n",
      "train loss:2.2274506484046377\n",
      "=== epoch:63, train acc:0.36666666666666664, test acc:0.2851 ===\n",
      "train loss:2.2292216062864085\n",
      "train loss:2.2107162751827363\n",
      "train loss:2.2353815882370465\n",
      "=== epoch:64, train acc:0.36333333333333334, test acc:0.2852 ===\n",
      "train loss:2.23207747352847\n",
      "train loss:2.229844289957893\n",
      "train loss:2.228070174662749\n",
      "=== epoch:65, train acc:0.36, test acc:0.2859 ===\n",
      "train loss:2.2291204559588977\n",
      "train loss:2.2175350291616143\n",
      "train loss:2.214944908591639\n",
      "=== epoch:66, train acc:0.36333333333333334, test acc:0.2912 ===\n",
      "train loss:2.191574827857122\n",
      "train loss:2.2325799619734625\n",
      "train loss:2.2293343467188023\n",
      "=== epoch:67, train acc:0.37666666666666665, test acc:0.2917 ===\n",
      "train loss:2.2279576894670985\n",
      "train loss:2.247415017690654\n",
      "train loss:2.2246854606420166\n",
      "=== epoch:68, train acc:0.38666666666666666, test acc:0.2959 ===\n",
      "train loss:2.2168104062903753\n",
      "train loss:2.2503946547643254\n",
      "train loss:2.2269703144246593\n",
      "=== epoch:69, train acc:0.38666666666666666, test acc:0.3007 ===\n",
      "train loss:2.2327313235540673\n",
      "train loss:2.209482552870237\n",
      "train loss:2.2288069242358772\n",
      "=== epoch:70, train acc:0.38333333333333336, test acc:0.2965 ===\n",
      "train loss:2.202347010718232\n",
      "train loss:2.1995049358093492\n",
      "train loss:2.225000443081901\n",
      "=== epoch:71, train acc:0.38333333333333336, test acc:0.2974 ===\n",
      "train loss:2.206849000598851\n",
      "train loss:2.228883037814412\n",
      "train loss:2.2019983151538356\n",
      "=== epoch:72, train acc:0.38333333333333336, test acc:0.2948 ===\n",
      "train loss:2.23442070371418\n",
      "train loss:2.227562485926402\n",
      "train loss:2.2172610677982165\n",
      "=== epoch:73, train acc:0.39, test acc:0.2986 ===\n",
      "train loss:2.207965153124992\n",
      "train loss:2.216875734768622\n",
      "train loss:2.200150637755066\n",
      "=== epoch:74, train acc:0.3933333333333333, test acc:0.3013 ===\n",
      "train loss:2.2022768566212663\n",
      "train loss:2.2111282799715464\n",
      "train loss:2.2105445539860797\n",
      "=== epoch:75, train acc:0.39, test acc:0.3014 ===\n",
      "train loss:2.210671212470307\n",
      "train loss:2.2071808438923664\n",
      "train loss:2.214146679334757\n",
      "=== epoch:76, train acc:0.39666666666666667, test acc:0.3049 ===\n",
      "train loss:2.200145822414086\n",
      "train loss:2.19539475056616\n",
      "train loss:2.1972039466308457\n",
      "=== epoch:77, train acc:0.4033333333333333, test acc:0.3055 ===\n",
      "train loss:2.2110377449742287\n",
      "train loss:2.21434179191133\n",
      "train loss:2.1909424663385137\n",
      "=== epoch:78, train acc:0.4033333333333333, test acc:0.309 ===\n",
      "train loss:2.2002659686614074\n",
      "train loss:2.2001724295898377\n",
      "train loss:2.2190809204666238\n",
      "=== epoch:79, train acc:0.4033333333333333, test acc:0.3095 ===\n",
      "train loss:2.195345654248529\n",
      "train loss:2.193724841587011\n",
      "train loss:2.1982454489977616\n",
      "=== epoch:80, train acc:0.4033333333333333, test acc:0.3109 ===\n",
      "train loss:2.176370926036487\n",
      "train loss:2.1555014023430177\n",
      "train loss:2.18586558385611\n",
      "=== epoch:81, train acc:0.4066666666666667, test acc:0.3046 ===\n",
      "train loss:2.2063741872901788\n",
      "train loss:2.1794594689025275\n",
      "train loss:2.2172655205866345\n",
      "=== epoch:82, train acc:0.39666666666666667, test acc:0.3078 ===\n",
      "train loss:2.1733157259693465\n",
      "train loss:2.1977182235622013\n",
      "train loss:2.204830191297228\n",
      "=== epoch:83, train acc:0.4066666666666667, test acc:0.3084 ===\n",
      "train loss:2.2083472337708128\n",
      "train loss:2.193212485495331\n",
      "train loss:2.1969532624204486\n",
      "=== epoch:84, train acc:0.39666666666666667, test acc:0.3112 ===\n",
      "train loss:2.1926787664689265\n",
      "train loss:2.21481624481805\n",
      "train loss:2.185971507224485\n",
      "=== epoch:85, train acc:0.4033333333333333, test acc:0.3128 ===\n",
      "train loss:2.1981750495651013\n",
      "train loss:2.169117760099127\n",
      "train loss:2.1738016557269275\n",
      "=== epoch:86, train acc:0.4033333333333333, test acc:0.3125 ===\n",
      "train loss:2.1807497434947627\n",
      "train loss:2.1712985304589516\n",
      "train loss:2.175869257307625\n",
      "=== epoch:87, train acc:0.4, test acc:0.3099 ===\n",
      "train loss:2.1833382291345984\n",
      "train loss:2.154365314104676\n",
      "train loss:2.191841542941352\n",
      "=== epoch:88, train acc:0.4033333333333333, test acc:0.3163 ===\n",
      "train loss:2.1516319744076697\n",
      "train loss:2.1709652353127735\n",
      "train loss:2.171449512155768\n",
      "=== epoch:89, train acc:0.4066666666666667, test acc:0.314 ===\n",
      "train loss:2.15611013161379\n",
      "train loss:2.1986004106070682\n",
      "train loss:2.196447612529065\n",
      "=== epoch:90, train acc:0.4166666666666667, test acc:0.3168 ===\n",
      "train loss:2.1894433415046306\n",
      "train loss:2.15175092678008\n",
      "train loss:2.2062004544420137\n",
      "=== epoch:91, train acc:0.4166666666666667, test acc:0.3158 ===\n",
      "train loss:2.1715206305967847\n",
      "train loss:2.1428282665414096\n",
      "train loss:2.1711791278211594\n",
      "=== epoch:92, train acc:0.4166666666666667, test acc:0.3145 ===\n",
      "train loss:2.1743362526845065\n",
      "train loss:2.194104352451517\n",
      "train loss:2.1981176298391096\n",
      "=== epoch:93, train acc:0.4166666666666667, test acc:0.3196 ===\n",
      "train loss:2.1422215379749505\n",
      "train loss:2.156736120072986\n",
      "train loss:2.1563912430457663\n",
      "=== epoch:94, train acc:0.42333333333333334, test acc:0.3218 ===\n",
      "train loss:2.1754355069540505\n",
      "train loss:2.1959707170317286\n",
      "train loss:2.187852027948019\n",
      "=== epoch:95, train acc:0.4266666666666667, test acc:0.3253 ===\n",
      "train loss:2.1765272605039647\n",
      "train loss:2.1772667313226113\n",
      "train loss:2.178829004962731\n",
      "=== epoch:96, train acc:0.43, test acc:0.3275 ===\n",
      "train loss:2.182178179892052\n",
      "train loss:2.152583510070272\n",
      "train loss:2.1576901446563186\n",
      "=== epoch:97, train acc:0.43333333333333335, test acc:0.3284 ===\n",
      "train loss:2.1982336241266687\n",
      "train loss:2.16011953213119\n",
      "train loss:2.1490981422830573\n",
      "=== epoch:98, train acc:0.43333333333333335, test acc:0.3294 ===\n",
      "train loss:2.169074802420641\n",
      "train loss:2.1713006329908238\n",
      "train loss:2.186237741448623\n",
      "=== epoch:99, train acc:0.44, test acc:0.3332 ===\n",
      "train loss:2.140464393988922\n",
      "train loss:2.1725595601810572\n",
      "train loss:2.1701832524372766\n",
      "=== epoch:100, train acc:0.44333333333333336, test acc:0.3323 ===\n",
      "train loss:2.1361413317024964\n",
      "train loss:2.1608676381125633\n",
      "train loss:2.120946128434248\n",
      "=== epoch:101, train acc:0.44666666666666666, test acc:0.3303 ===\n",
      "train loss:2.093451448510136\n",
      "train loss:2.177848773403695\n",
      "train loss:2.189899940034731\n",
      "=== epoch:102, train acc:0.45, test acc:0.331 ===\n",
      "train loss:2.1353677357300684\n",
      "train loss:2.1191253527222402\n",
      "train loss:2.1642778078387193\n",
      "=== epoch:103, train acc:0.4533333333333333, test acc:0.3322 ===\n",
      "train loss:2.1269886694166447\n",
      "train loss:2.14634743810623\n",
      "train loss:2.15426064156177\n",
      "=== epoch:104, train acc:0.44666666666666666, test acc:0.3343 ===\n",
      "train loss:2.1993252219155766\n",
      "train loss:2.1382711451768794\n",
      "train loss:2.1406604994897376\n",
      "=== epoch:105, train acc:0.4533333333333333, test acc:0.3348 ===\n",
      "train loss:2.1327826583097123\n",
      "train loss:2.1683413644344545\n",
      "train loss:2.1516379555918466\n",
      "=== epoch:106, train acc:0.4633333333333333, test acc:0.3384 ===\n",
      "train loss:2.156943810494776\n",
      "train loss:2.0986129516095193\n",
      "train loss:2.1256008531915147\n",
      "=== epoch:107, train acc:0.4533333333333333, test acc:0.3352 ===\n",
      "train loss:2.157852299120074\n",
      "train loss:2.141693205905323\n",
      "train loss:2.154728756926794\n",
      "=== epoch:108, train acc:0.4533333333333333, test acc:0.3376 ===\n",
      "train loss:2.121607562130792\n",
      "train loss:2.115371446998525\n",
      "train loss:2.144846194553234\n",
      "=== epoch:109, train acc:0.45, test acc:0.3396 ===\n",
      "train loss:2.1155167514064175\n",
      "train loss:2.1086054022901615\n",
      "train loss:2.1041406106059144\n",
      "=== epoch:110, train acc:0.45, test acc:0.3367 ===\n",
      "train loss:2.093507797764245\n",
      "train loss:2.127169151315452\n",
      "train loss:2.106747813468454\n",
      "=== epoch:111, train acc:0.45, test acc:0.3396 ===\n",
      "train loss:2.1150103971962144\n",
      "train loss:2.1323223907824755\n",
      "train loss:2.1554638626882188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:112, train acc:0.45666666666666667, test acc:0.3427 ===\n",
      "train loss:2.100858858831559\n",
      "train loss:2.1654561627601185\n",
      "train loss:2.1593025600586966\n",
      "=== epoch:113, train acc:0.4633333333333333, test acc:0.3454 ===\n",
      "train loss:2.138889305149236\n",
      "train loss:2.1399917423079247\n",
      "train loss:2.1025369292863845\n",
      "=== epoch:114, train acc:0.47, test acc:0.3463 ===\n",
      "train loss:2.117123707957264\n",
      "train loss:2.0806533828203877\n",
      "train loss:2.0795013973065894\n",
      "=== epoch:115, train acc:0.47, test acc:0.3441 ===\n",
      "train loss:2.089235147907476\n",
      "train loss:2.1094477798936753\n",
      "train loss:2.061497636851043\n",
      "=== epoch:116, train acc:0.45666666666666667, test acc:0.341 ===\n",
      "train loss:2.1237255709233263\n",
      "train loss:2.1288858612785004\n",
      "train loss:2.120238383763665\n",
      "=== epoch:117, train acc:0.45666666666666667, test acc:0.3432 ===\n",
      "train loss:2.1336441713968326\n",
      "train loss:2.0735420978279437\n",
      "train loss:2.131829190877404\n",
      "=== epoch:118, train acc:0.46, test acc:0.3441 ===\n",
      "train loss:2.129251359305799\n",
      "train loss:2.122949848847102\n",
      "train loss:2.072848468771883\n",
      "=== epoch:119, train acc:0.46, test acc:0.3453 ===\n",
      "train loss:2.138550332800038\n",
      "train loss:2.1463575705005415\n",
      "train loss:2.122766455413669\n",
      "=== epoch:120, train acc:0.44, test acc:0.3488 ===\n",
      "train loss:2.0338017928514436\n",
      "train loss:2.0771824679935156\n",
      "train loss:2.087690415864919\n",
      "=== epoch:121, train acc:0.43666666666666665, test acc:0.3465 ===\n",
      "train loss:2.105358439037318\n",
      "train loss:2.068443355954268\n",
      "train loss:2.124798772539042\n",
      "=== epoch:122, train acc:0.43666666666666665, test acc:0.3467 ===\n",
      "train loss:2.056293106006333\n",
      "train loss:2.104985272015358\n",
      "train loss:2.0287237632067376\n",
      "=== epoch:123, train acc:0.43666666666666665, test acc:0.3466 ===\n",
      "train loss:2.0904682473631624\n",
      "train loss:2.096909891377396\n",
      "train loss:2.093796766184992\n",
      "=== epoch:124, train acc:0.43333333333333335, test acc:0.3448 ===\n",
      "train loss:2.0637905326686745\n",
      "train loss:2.0590729922070246\n",
      "train loss:2.0552813036326736\n",
      "=== epoch:125, train acc:0.4266666666666667, test acc:0.3426 ===\n",
      "train loss:2.093238827971415\n",
      "train loss:2.064279690971278\n",
      "train loss:2.0213816516859535\n",
      "=== epoch:126, train acc:0.4266666666666667, test acc:0.3434 ===\n",
      "train loss:2.1298668183125637\n",
      "train loss:2.045207882144757\n",
      "train loss:2.097449885624617\n",
      "=== epoch:127, train acc:0.43, test acc:0.345 ===\n",
      "train loss:2.104178102580699\n",
      "train loss:2.0608948796438065\n",
      "train loss:2.04054902227507\n",
      "=== epoch:128, train acc:0.4166666666666667, test acc:0.3437 ===\n",
      "train loss:2.03678334904464\n",
      "train loss:2.060586426641954\n",
      "train loss:2.0820029619961833\n",
      "=== epoch:129, train acc:0.42333333333333334, test acc:0.344 ===\n",
      "train loss:2.1051223025964436\n",
      "train loss:2.0293190004538895\n",
      "train loss:2.0677969674056844\n",
      "=== epoch:130, train acc:0.42, test acc:0.3445 ===\n",
      "train loss:2.0249110209473313\n",
      "train loss:2.093909846410379\n",
      "train loss:2.036133418768653\n",
      "=== epoch:131, train acc:0.4033333333333333, test acc:0.3387 ===\n",
      "train loss:2.068026952782773\n",
      "train loss:2.0955478301185972\n",
      "train loss:2.0353183633521272\n",
      "=== epoch:132, train acc:0.4266666666666667, test acc:0.3449 ===\n",
      "train loss:2.0377282895775877\n",
      "train loss:2.0632697121388976\n",
      "train loss:2.0684293287976825\n",
      "=== epoch:133, train acc:0.42, test acc:0.3463 ===\n",
      "train loss:2.0473079860675196\n",
      "train loss:2.021491845215669\n",
      "train loss:2.0532722958286214\n",
      "=== epoch:134, train acc:0.4266666666666667, test acc:0.3448 ===\n",
      "train loss:2.0592040508795724\n",
      "train loss:2.0080894951461516\n",
      "train loss:2.0053790083133047\n",
      "=== epoch:135, train acc:0.43333333333333335, test acc:0.3487 ===\n",
      "train loss:2.0204128312058587\n",
      "train loss:1.9770037980778428\n",
      "train loss:2.0181661124549155\n",
      "=== epoch:136, train acc:0.43, test acc:0.3484 ===\n",
      "train loss:2.1024465643601182\n",
      "train loss:2.0246746549981416\n",
      "train loss:2.0063183674139977\n",
      "=== epoch:137, train acc:0.43333333333333335, test acc:0.3493 ===\n",
      "train loss:2.0391617477099473\n",
      "train loss:2.0184343001118608\n",
      "train loss:2.01507041927567\n",
      "=== epoch:138, train acc:0.42333333333333334, test acc:0.3513 ===\n",
      "train loss:2.044200298192521\n",
      "train loss:2.001462777392194\n",
      "train loss:2.0257427418191476\n",
      "=== epoch:139, train acc:0.42, test acc:0.3464 ===\n",
      "train loss:1.987074478640042\n",
      "train loss:2.0101754972505934\n",
      "train loss:2.042781909489915\n",
      "=== epoch:140, train acc:0.4266666666666667, test acc:0.3485 ===\n",
      "train loss:1.9529952602707423\n",
      "train loss:2.003418692825883\n",
      "train loss:1.9815517100521705\n",
      "=== epoch:141, train acc:0.43333333333333335, test acc:0.3496 ===\n",
      "train loss:2.03796403412153\n",
      "train loss:2.086978396292128\n",
      "train loss:2.0239564517946222\n",
      "=== epoch:142, train acc:0.43666666666666665, test acc:0.3523 ===\n",
      "train loss:2.003340299020584\n",
      "train loss:1.9983622313598903\n",
      "train loss:2.032040629600058\n",
      "=== epoch:143, train acc:0.44, test acc:0.3571 ===\n",
      "train loss:1.9870617150525782\n",
      "train loss:1.9822250411459512\n",
      "train loss:2.0559072332609403\n",
      "=== epoch:144, train acc:0.43333333333333335, test acc:0.3532 ===\n",
      "train loss:1.9497881660545098\n",
      "train loss:2.000657566996388\n",
      "train loss:1.9354162998395332\n",
      "=== epoch:145, train acc:0.43, test acc:0.355 ===\n",
      "train loss:1.946404305997411\n",
      "train loss:1.9422028863022347\n",
      "train loss:2.0085843465284667\n",
      "=== epoch:146, train acc:0.43, test acc:0.35 ===\n",
      "train loss:1.988619156137967\n",
      "train loss:1.9513403977056532\n",
      "train loss:1.944974751503137\n",
      "=== epoch:147, train acc:0.42, test acc:0.3433 ===\n",
      "train loss:2.0244873801460743\n",
      "train loss:1.9286512256640627\n",
      "train loss:1.879352513596494\n",
      "=== epoch:148, train acc:0.4166666666666667, test acc:0.3414 ===\n",
      "train loss:1.9056814517843947\n",
      "train loss:2.0412769348915227\n",
      "train loss:1.9656602316201583\n",
      "=== epoch:149, train acc:0.42, test acc:0.3453 ===\n",
      "train loss:1.9483359060116472\n",
      "train loss:1.9598043589937475\n",
      "train loss:1.9842223405132557\n",
      "=== epoch:150, train acc:0.4266666666666667, test acc:0.3476 ===\n",
      "train loss:1.9862409894242874\n",
      "train loss:2.0192973851828158\n",
      "train loss:1.9684178903087093\n",
      "=== epoch:151, train acc:0.43333333333333335, test acc:0.3525 ===\n",
      "train loss:1.9636656438010405\n",
      "train loss:1.9125223748466162\n",
      "train loss:1.9171036401988724\n",
      "=== epoch:152, train acc:0.43666666666666665, test acc:0.3524 ===\n",
      "train loss:2.007197127824691\n",
      "train loss:1.873755300120547\n",
      "train loss:1.9183045085565253\n",
      "=== epoch:153, train acc:0.4266666666666667, test acc:0.3524 ===\n",
      "train loss:2.037928805687602\n",
      "train loss:1.8917277790959737\n",
      "train loss:1.928523413171699\n",
      "=== epoch:154, train acc:0.44, test acc:0.3582 ===\n",
      "train loss:1.926678179400312\n",
      "train loss:1.924435401066763\n",
      "train loss:1.9563046335523055\n",
      "=== epoch:155, train acc:0.43666666666666665, test acc:0.3614 ===\n",
      "train loss:1.892559879396259\n",
      "train loss:1.991981361816408\n",
      "train loss:1.9145864669436514\n",
      "=== epoch:156, train acc:0.44333333333333336, test acc:0.3592 ===\n",
      "train loss:1.9389126751346033\n",
      "train loss:1.9244587037059753\n",
      "train loss:1.916548867749715\n",
      "=== epoch:157, train acc:0.44666666666666666, test acc:0.3568 ===\n",
      "train loss:1.8831582436964904\n",
      "train loss:1.9238086207682497\n",
      "train loss:1.9322460237860934\n",
      "=== epoch:158, train acc:0.45, test acc:0.3599 ===\n",
      "train loss:1.8772782203592266\n",
      "train loss:1.9422587234207491\n",
      "train loss:1.932575509508717\n",
      "=== epoch:159, train acc:0.4533333333333333, test acc:0.3642 ===\n",
      "train loss:1.9384772033291116\n",
      "train loss:1.9471819426799342\n",
      "train loss:1.9239469868311443\n",
      "=== epoch:160, train acc:0.45, test acc:0.365 ===\n",
      "train loss:1.936514021115772\n",
      "train loss:1.9075108795856393\n",
      "train loss:1.9098170900755096\n",
      "=== epoch:161, train acc:0.44666666666666666, test acc:0.3673 ===\n",
      "train loss:1.9381479695929988\n",
      "train loss:1.8549080417425465\n",
      "train loss:1.8594134656055468\n",
      "=== epoch:162, train acc:0.4533333333333333, test acc:0.3673 ===\n",
      "train loss:1.8860897996770498\n",
      "train loss:1.9385393113401912\n",
      "train loss:1.9147010418082553\n",
      "=== epoch:163, train acc:0.4666666666666667, test acc:0.3768 ===\n",
      "train loss:1.8636581274515214\n",
      "train loss:1.7798571729485149\n",
      "train loss:1.8534124893703319\n",
      "=== epoch:164, train acc:0.47, test acc:0.3731 ===\n",
      "train loss:1.8863631094517859\n",
      "train loss:1.845086274742063\n",
      "train loss:1.9034718905680321\n",
      "=== epoch:165, train acc:0.4633333333333333, test acc:0.3727 ===\n",
      "train loss:1.8817538427398879\n",
      "train loss:1.854071615251243\n",
      "train loss:1.882177130259904\n",
      "=== epoch:166, train acc:0.4766666666666667, test acc:0.3817 ===\n",
      "train loss:1.890091311462449\n",
      "train loss:1.8913028958976468\n",
      "train loss:1.8867221239757177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:167, train acc:0.48333333333333334, test acc:0.3775 ===\n",
      "train loss:1.874748899890094\n",
      "train loss:1.7659383739749277\n",
      "train loss:1.7935351330152036\n",
      "=== epoch:168, train acc:0.4766666666666667, test acc:0.3713 ===\n",
      "train loss:1.8877017754235907\n",
      "train loss:1.8927754984955507\n",
      "train loss:1.8504987962391584\n",
      "=== epoch:169, train acc:0.48, test acc:0.3771 ===\n",
      "train loss:1.86668919907939\n",
      "train loss:1.9675765333018453\n",
      "train loss:1.8915193533040675\n",
      "=== epoch:170, train acc:0.48333333333333334, test acc:0.3814 ===\n",
      "train loss:1.907201836372975\n",
      "train loss:1.8202698828379267\n",
      "train loss:1.9353060905857742\n",
      "=== epoch:171, train acc:0.49333333333333335, test acc:0.3885 ===\n",
      "train loss:1.8277419199032492\n",
      "train loss:1.9408342878861788\n",
      "train loss:1.8938570671600001\n",
      "=== epoch:172, train acc:0.5, test acc:0.3916 ===\n",
      "train loss:1.8729430780022707\n",
      "train loss:1.9170694231738812\n",
      "train loss:1.8458608726283636\n",
      "=== epoch:173, train acc:0.49666666666666665, test acc:0.3908 ===\n",
      "train loss:1.954135878020374\n",
      "train loss:1.7807748114119126\n",
      "train loss:1.8502908781117904\n",
      "=== epoch:174, train acc:0.49333333333333335, test acc:0.3903 ===\n",
      "train loss:1.8314819635453328\n",
      "train loss:1.7947459371337364\n",
      "train loss:1.706012727557221\n",
      "=== epoch:175, train acc:0.5, test acc:0.3909 ===\n",
      "train loss:1.872905431250759\n",
      "train loss:1.8921594203864933\n",
      "train loss:1.8267521334775805\n",
      "=== epoch:176, train acc:0.49666666666666665, test acc:0.3906 ===\n",
      "train loss:1.8112133183265922\n",
      "train loss:1.7890321131727231\n",
      "train loss:1.7942420888022446\n",
      "=== epoch:177, train acc:0.5133333333333333, test acc:0.3978 ===\n",
      "train loss:1.8760426331406896\n",
      "train loss:1.8223570422364481\n",
      "train loss:1.6974960166744553\n",
      "=== epoch:178, train acc:0.5166666666666667, test acc:0.4 ===\n",
      "train loss:1.8322766984872498\n",
      "train loss:1.7905177775828802\n",
      "train loss:1.7465076117857905\n",
      "=== epoch:179, train acc:0.5233333333333333, test acc:0.4062 ===\n",
      "train loss:1.8601012566690156\n",
      "train loss:1.7846195119955042\n",
      "train loss:1.8184373314636333\n",
      "=== epoch:180, train acc:0.5266666666666666, test acc:0.4092 ===\n",
      "train loss:1.8845017205692105\n",
      "train loss:1.940003398904272\n",
      "train loss:1.796958695405105\n",
      "=== epoch:181, train acc:0.5266666666666666, test acc:0.4105 ===\n",
      "train loss:1.7640832376148545\n",
      "train loss:1.7849392113862472\n",
      "train loss:1.8489940829462332\n",
      "=== epoch:182, train acc:0.5333333333333333, test acc:0.4122 ===\n",
      "train loss:1.8694786947642006\n",
      "train loss:1.79643330873851\n",
      "train loss:1.771029815710306\n",
      "=== epoch:183, train acc:0.5266666666666666, test acc:0.4119 ===\n",
      "train loss:1.8042766179122347\n",
      "train loss:1.7037631069195844\n",
      "train loss:1.7913320695742911\n",
      "=== epoch:184, train acc:0.53, test acc:0.4119 ===\n",
      "train loss:1.775506095651488\n",
      "train loss:1.7929495515779936\n",
      "train loss:1.6130213305638252\n",
      "=== epoch:185, train acc:0.51, test acc:0.4025 ===\n",
      "train loss:1.710451303591524\n",
      "train loss:1.7952385386497818\n",
      "train loss:1.8062653576210963\n",
      "=== epoch:186, train acc:0.5133333333333333, test acc:0.4063 ===\n",
      "train loss:1.825859035118841\n",
      "train loss:1.7223324649565708\n",
      "train loss:1.7212963104013463\n",
      "=== epoch:187, train acc:0.5166666666666667, test acc:0.4106 ===\n",
      "train loss:1.8248692309893335\n",
      "train loss:1.8012347437339473\n",
      "train loss:1.7539688934458295\n",
      "=== epoch:188, train acc:0.5166666666666667, test acc:0.4107 ===\n",
      "train loss:1.756989185076229\n",
      "train loss:1.8030335756763705\n",
      "train loss:1.6947388975932358\n",
      "=== epoch:189, train acc:0.52, test acc:0.4092 ===\n",
      "train loss:1.667397700203582\n",
      "train loss:1.8496231182803593\n",
      "train loss:1.614529510045391\n",
      "=== epoch:190, train acc:0.52, test acc:0.4093 ===\n",
      "train loss:1.732072663024329\n",
      "train loss:1.710744912133219\n",
      "train loss:1.750866184578747\n",
      "=== epoch:191, train acc:0.5266666666666666, test acc:0.4092 ===\n",
      "train loss:1.7776052681463037\n",
      "train loss:1.8180214541540756\n",
      "train loss:1.7166641893346068\n",
      "=== epoch:192, train acc:0.52, test acc:0.4092 ===\n",
      "train loss:1.7287438259391548\n",
      "train loss:1.7605740260879656\n",
      "train loss:1.66291084159269\n",
      "=== epoch:193, train acc:0.5233333333333333, test acc:0.4094 ===\n",
      "train loss:1.7623718516624494\n",
      "train loss:1.7845131764123698\n",
      "train loss:1.715683996082115\n",
      "=== epoch:194, train acc:0.5333333333333333, test acc:0.4126 ===\n",
      "train loss:1.6813196272850837\n",
      "train loss:1.7145146591711689\n",
      "train loss:1.7810665728705946\n",
      "=== epoch:195, train acc:0.5166666666666667, test acc:0.4089 ===\n",
      "train loss:1.7260300274624816\n",
      "train loss:1.7159939425166661\n",
      "train loss:1.678465326824394\n",
      "=== epoch:196, train acc:0.52, test acc:0.415 ===\n",
      "train loss:1.7151936126256695\n",
      "train loss:1.6876919388397598\n",
      "train loss:1.7722660965542036\n",
      "=== epoch:197, train acc:0.5233333333333333, test acc:0.415 ===\n",
      "train loss:1.724800446973259\n",
      "train loss:1.7142352710627589\n",
      "train loss:1.6164164428336791\n",
      "=== epoch:198, train acc:0.54, test acc:0.4226 ===\n",
      "train loss:1.7336742796483968\n",
      "train loss:1.6349338040543413\n",
      "train loss:1.5944103789185775\n",
      "=== epoch:199, train acc:0.55, test acc:0.4299 ===\n",
      "train loss:1.73875208532982\n",
      "train loss:1.6449150519590094\n",
      "train loss:1.722889807458197\n",
      "=== epoch:200, train acc:0.5466666666666666, test acc:0.4301 ===\n",
      "train loss:1.7788334335236715\n",
      "train loss:1.6998655731244643\n",
      "train loss:1.7012098672062228\n",
      "=== epoch:201, train acc:0.5466666666666666, test acc:0.4302 ===\n",
      "train loss:1.7193593078438143\n",
      "train loss:1.80598143055824\n",
      "train loss:1.5808899886921375\n",
      "=== epoch:202, train acc:0.5366666666666666, test acc:0.4221 ===\n",
      "train loss:1.6753364941332922\n",
      "train loss:1.66941299202334\n",
      "train loss:1.617585803450755\n",
      "=== epoch:203, train acc:0.55, test acc:0.4262 ===\n",
      "train loss:1.7513228368676417\n",
      "train loss:1.591048864110567\n",
      "train loss:1.7007935258462104\n",
      "=== epoch:204, train acc:0.55, test acc:0.4265 ===\n",
      "train loss:1.667857048132115\n",
      "train loss:1.661395813255909\n",
      "train loss:1.667674231747471\n",
      "=== epoch:205, train acc:0.55, test acc:0.4272 ===\n",
      "train loss:1.626734603507794\n",
      "train loss:1.7059031483292608\n",
      "train loss:1.6121138785610927\n",
      "=== epoch:206, train acc:0.55, test acc:0.4256 ===\n",
      "train loss:1.6402987477844135\n",
      "train loss:1.6868082093569767\n",
      "train loss:1.656332486762124\n",
      "=== epoch:207, train acc:0.5633333333333334, test acc:0.4342 ===\n",
      "train loss:1.7135068799389328\n",
      "train loss:1.6822144434301853\n",
      "train loss:1.5126334405692428\n",
      "=== epoch:208, train acc:0.5566666666666666, test acc:0.4326 ===\n",
      "train loss:1.749916602255012\n",
      "train loss:1.6708316023655252\n",
      "train loss:1.6143622688694155\n",
      "=== epoch:209, train acc:0.5666666666666667, test acc:0.4387 ===\n",
      "train loss:1.6530936195520676\n",
      "train loss:1.6734170929360035\n",
      "train loss:1.614938611209868\n",
      "=== epoch:210, train acc:0.56, test acc:0.4404 ===\n",
      "train loss:1.607165819663596\n",
      "train loss:1.6069499152223785\n",
      "train loss:1.693726717718636\n",
      "=== epoch:211, train acc:0.5766666666666667, test acc:0.4468 ===\n",
      "train loss:1.6595242440099764\n",
      "train loss:1.5283650801592512\n",
      "train loss:1.7323143404201775\n",
      "=== epoch:212, train acc:0.5766666666666667, test acc:0.4487 ===\n",
      "train loss:1.7033396427875522\n",
      "train loss:1.6618662223950702\n",
      "train loss:1.711316221184341\n",
      "=== epoch:213, train acc:0.5933333333333334, test acc:0.4533 ===\n",
      "train loss:1.7086166845285586\n",
      "train loss:1.6143364195768908\n",
      "train loss:1.5749023679383394\n",
      "=== epoch:214, train acc:0.6033333333333334, test acc:0.4559 ===\n",
      "train loss:1.653161658474564\n",
      "train loss:1.6751207854523964\n",
      "train loss:1.5414592179460334\n",
      "=== epoch:215, train acc:0.6066666666666667, test acc:0.4568 ===\n",
      "train loss:1.6020809783152246\n",
      "train loss:1.5807605085378749\n",
      "train loss:1.472308766572139\n",
      "=== epoch:216, train acc:0.5833333333333334, test acc:0.4495 ===\n",
      "train loss:1.6572763171287619\n",
      "train loss:1.681390101306335\n",
      "train loss:1.5651982010435859\n",
      "=== epoch:217, train acc:0.5966666666666667, test acc:0.4491 ===\n",
      "train loss:1.4372929599688364\n",
      "train loss:1.6606129424062908\n",
      "train loss:1.522334432154305\n",
      "=== epoch:218, train acc:0.57, test acc:0.4429 ===\n",
      "train loss:1.644807252345951\n",
      "train loss:1.5931537248701357\n",
      "train loss:1.6751261729695341\n",
      "=== epoch:219, train acc:0.5633333333333334, test acc:0.4394 ===\n",
      "train loss:1.607684763407788\n",
      "train loss:1.4701323547094418\n",
      "train loss:1.538115798638212\n",
      "=== epoch:220, train acc:0.5766666666666667, test acc:0.4437 ===\n",
      "train loss:1.6678556723409812\n",
      "train loss:1.5669224233034273\n",
      "train loss:1.5680835457609792\n",
      "=== epoch:221, train acc:0.5666666666666667, test acc:0.4408 ===\n",
      "train loss:1.6293594869460752\n",
      "train loss:1.624691739920084\n",
      "train loss:1.6668510792385995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:222, train acc:0.5766666666666667, test acc:0.4457 ===\n",
      "train loss:1.6076323343306953\n",
      "train loss:1.4697900329789955\n",
      "train loss:1.5169273773567633\n",
      "=== epoch:223, train acc:0.5833333333333334, test acc:0.4481 ===\n",
      "train loss:1.6208782338241194\n",
      "train loss:1.5876987744384017\n",
      "train loss:1.5634455160634235\n",
      "=== epoch:224, train acc:0.6033333333333334, test acc:0.4595 ===\n",
      "train loss:1.468039759783743\n",
      "train loss:1.5939892899172592\n",
      "train loss:1.5035344419808405\n",
      "=== epoch:225, train acc:0.6166666666666667, test acc:0.4651 ===\n",
      "train loss:1.420268387242489\n",
      "train loss:1.4984743053715277\n",
      "train loss:1.5566585128719934\n",
      "=== epoch:226, train acc:0.6233333333333333, test acc:0.4717 ===\n",
      "train loss:1.5226656485112633\n",
      "train loss:1.490504003937304\n",
      "train loss:1.5148253495942268\n",
      "=== epoch:227, train acc:0.6266666666666667, test acc:0.4739 ===\n",
      "train loss:1.6543221712721359\n",
      "train loss:1.5742862310844257\n",
      "train loss:1.6262098561357596\n",
      "=== epoch:228, train acc:0.6333333333333333, test acc:0.4782 ===\n",
      "train loss:1.6387401989313477\n",
      "train loss:1.5071661949235664\n",
      "train loss:1.5199580849330332\n",
      "=== epoch:229, train acc:0.63, test acc:0.4752 ===\n",
      "train loss:1.5911826262900106\n",
      "train loss:1.5890599588597132\n",
      "train loss:1.6423437124054139\n",
      "=== epoch:230, train acc:0.62, test acc:0.4679 ===\n",
      "train loss:1.5230066283082007\n",
      "train loss:1.5360437931001403\n",
      "train loss:1.596194774298747\n",
      "=== epoch:231, train acc:0.6366666666666667, test acc:0.4697 ===\n",
      "train loss:1.523806038805476\n",
      "train loss:1.641735505526383\n",
      "train loss:1.4781218225203034\n",
      "=== epoch:232, train acc:0.65, test acc:0.4729 ===\n",
      "train loss:1.5064917319861248\n",
      "train loss:1.488083873233432\n",
      "train loss:1.564102927072239\n",
      "=== epoch:233, train acc:0.6266666666666667, test acc:0.4683 ===\n",
      "train loss:1.5023700605769092\n",
      "train loss:1.5420158917023365\n",
      "train loss:1.5918303001826586\n",
      "=== epoch:234, train acc:0.6366666666666667, test acc:0.4712 ===\n",
      "train loss:1.4728769408466291\n",
      "train loss:1.508301673710265\n",
      "train loss:1.5040407544354257\n",
      "=== epoch:235, train acc:0.63, test acc:0.4695 ===\n",
      "train loss:1.5071958589689032\n",
      "train loss:1.4837444439875072\n",
      "train loss:1.4711459004719072\n",
      "=== epoch:236, train acc:0.6266666666666667, test acc:0.47 ===\n",
      "train loss:1.4957721223100358\n",
      "train loss:1.6130057971901453\n",
      "train loss:1.5622718894397725\n",
      "=== epoch:237, train acc:0.6466666666666666, test acc:0.4768 ===\n",
      "train loss:1.5354998504319806\n",
      "train loss:1.4385525491796336\n",
      "train loss:1.5419526193305058\n",
      "=== epoch:238, train acc:0.65, test acc:0.4816 ===\n",
      "train loss:1.554531867720349\n",
      "train loss:1.5864780490813437\n",
      "train loss:1.634563520850243\n",
      "=== epoch:239, train acc:0.6633333333333333, test acc:0.4799 ===\n",
      "train loss:1.3090424327734518\n",
      "train loss:1.4954942171561776\n",
      "train loss:1.5741800192862412\n",
      "=== epoch:240, train acc:0.6633333333333333, test acc:0.4813 ===\n",
      "train loss:1.5749204205389795\n",
      "train loss:1.552107951405575\n",
      "train loss:1.4871684536960004\n",
      "=== epoch:241, train acc:0.6766666666666666, test acc:0.4851 ===\n",
      "train loss:1.4928029560243161\n",
      "train loss:1.5790998550064705\n",
      "train loss:1.426567875681279\n",
      "=== epoch:242, train acc:0.6766666666666666, test acc:0.4847 ===\n",
      "train loss:1.5405122304024945\n",
      "train loss:1.4345840602307605\n",
      "train loss:1.3441758126357315\n",
      "=== epoch:243, train acc:0.6733333333333333, test acc:0.483 ===\n",
      "train loss:1.3225677741969262\n",
      "train loss:1.60329365998189\n",
      "train loss:1.489488762630931\n",
      "=== epoch:244, train acc:0.69, test acc:0.4888 ===\n",
      "train loss:1.4451841063033262\n",
      "train loss:1.5049093132469367\n",
      "train loss:1.485508009830881\n",
      "=== epoch:245, train acc:0.6833333333333333, test acc:0.4835 ===\n",
      "train loss:1.49682493472329\n",
      "train loss:1.366989446870155\n",
      "train loss:1.4450531246556104\n",
      "=== epoch:246, train acc:0.6666666666666666, test acc:0.4798 ===\n",
      "train loss:1.4307987195818657\n",
      "train loss:1.5614941990925741\n",
      "train loss:1.4398751032661614\n",
      "=== epoch:247, train acc:0.6766666666666666, test acc:0.4839 ===\n",
      "train loss:1.4512426852297782\n",
      "train loss:1.4737058936266754\n",
      "train loss:1.4144195441523033\n",
      "=== epoch:248, train acc:0.68, test acc:0.4836 ===\n",
      "train loss:1.5485759055673696\n",
      "train loss:1.4393824335391465\n",
      "train loss:1.3887021505503634\n",
      "=== epoch:249, train acc:0.68, test acc:0.4849 ===\n",
      "train loss:1.5082583754177954\n",
      "train loss:1.4987316321062893\n",
      "train loss:1.3893815882118112\n",
      "=== epoch:250, train acc:0.66, test acc:0.4775 ===\n",
      "train loss:1.450226626805987\n",
      "train loss:1.431381488757711\n",
      "train loss:1.482792934979712\n",
      "=== epoch:251, train acc:0.6466666666666666, test acc:0.4776 ===\n",
      "train loss:1.4280286486100542\n",
      "train loss:1.4159288134461352\n",
      "train loss:1.5844688652762435\n",
      "=== epoch:252, train acc:0.66, test acc:0.4784 ===\n",
      "train loss:1.502645853250552\n",
      "train loss:1.395040589400694\n",
      "train loss:1.4961090846754999\n",
      "=== epoch:253, train acc:0.6766666666666666, test acc:0.4834 ===\n",
      "train loss:1.403291204697413\n",
      "train loss:1.4190945190783741\n",
      "train loss:1.4185468972246453\n",
      "=== epoch:254, train acc:0.7033333333333334, test acc:0.4921 ===\n",
      "train loss:1.6068980637788446\n",
      "train loss:1.4161428240632983\n",
      "train loss:1.5362933191066332\n",
      "=== epoch:255, train acc:0.71, test acc:0.4985 ===\n",
      "train loss:1.3506636052945415\n",
      "train loss:1.4190361639061442\n",
      "train loss:1.3669624005383116\n",
      "=== epoch:256, train acc:0.6933333333333334, test acc:0.4964 ===\n",
      "train loss:1.31662020336687\n",
      "train loss:1.4171691906856159\n",
      "train loss:1.4161083404467767\n",
      "=== epoch:257, train acc:0.6933333333333334, test acc:0.4914 ===\n",
      "train loss:1.4200850026521648\n",
      "train loss:1.5041193322433468\n",
      "train loss:1.4101777217803098\n",
      "=== epoch:258, train acc:0.7, test acc:0.493 ===\n",
      "train loss:1.289015874996943\n",
      "train loss:1.3465551496136274\n",
      "train loss:1.4548066956442767\n",
      "=== epoch:259, train acc:0.7, test acc:0.4965 ===\n",
      "train loss:1.4986707449352827\n",
      "train loss:1.5025184661065674\n",
      "train loss:1.528845466564053\n",
      "=== epoch:260, train acc:0.6966666666666667, test acc:0.5027 ===\n",
      "train loss:1.3619349400117071\n",
      "train loss:1.503147854364701\n",
      "train loss:1.4084630856729756\n",
      "=== epoch:261, train acc:0.71, test acc:0.5093 ===\n",
      "train loss:1.498700284738669\n",
      "train loss:1.3561225251359577\n",
      "train loss:1.3065972380954627\n",
      "=== epoch:262, train acc:0.7066666666666667, test acc:0.5081 ===\n",
      "train loss:1.4274774789151163\n",
      "train loss:1.4391524077464088\n",
      "train loss:1.4064948900419787\n",
      "=== epoch:263, train acc:0.7133333333333334, test acc:0.5113 ===\n",
      "train loss:1.30988020405996\n",
      "train loss:1.3643663242016273\n",
      "train loss:1.2829884152255164\n",
      "=== epoch:264, train acc:0.7033333333333334, test acc:0.5075 ===\n",
      "train loss:1.3497781971893394\n",
      "train loss:1.202018887684645\n",
      "train loss:1.378354769334318\n",
      "=== epoch:265, train acc:0.7, test acc:0.5113 ===\n",
      "train loss:1.4465894406801205\n",
      "train loss:1.3813562113604314\n",
      "train loss:1.2948578496076777\n",
      "=== epoch:266, train acc:0.7033333333333334, test acc:0.5152 ===\n",
      "train loss:1.3293073940643962\n",
      "train loss:1.4030922795949285\n",
      "train loss:1.3286779779003086\n",
      "=== epoch:267, train acc:0.7166666666666667, test acc:0.5161 ===\n",
      "train loss:1.4647638401698324\n",
      "train loss:1.2788354404352156\n",
      "train loss:1.363978460627507\n",
      "=== epoch:268, train acc:0.72, test acc:0.52 ===\n",
      "train loss:1.476748350957073\n",
      "train loss:1.3153045363946452\n",
      "train loss:1.3827728805752448\n",
      "=== epoch:269, train acc:0.7166666666666667, test acc:0.5155 ===\n",
      "train loss:1.338097881040123\n",
      "train loss:1.2022325834939014\n",
      "train loss:1.4860159660591916\n",
      "=== epoch:270, train acc:0.7266666666666667, test acc:0.5147 ===\n",
      "train loss:1.3996765692010242\n",
      "train loss:1.3370371762512123\n",
      "train loss:1.2034684436584917\n",
      "=== epoch:271, train acc:0.7266666666666667, test acc:0.5164 ===\n",
      "train loss:1.3558932899118066\n",
      "train loss:1.2649813692890104\n",
      "train loss:1.3220440195191392\n",
      "=== epoch:272, train acc:0.7233333333333334, test acc:0.517 ===\n",
      "train loss:1.2568229054287727\n",
      "train loss:1.3977435280138608\n",
      "train loss:1.3771101011706575\n",
      "=== epoch:273, train acc:0.72, test acc:0.5158 ===\n",
      "train loss:1.3601138051849098\n",
      "train loss:1.2841293654985326\n",
      "train loss:1.283207988461085\n",
      "=== epoch:274, train acc:0.7266666666666667, test acc:0.5177 ===\n",
      "train loss:1.221832560945427\n",
      "train loss:1.2628719683982788\n",
      "train loss:1.1967930307692982\n",
      "=== epoch:275, train acc:0.7366666666666667, test acc:0.5186 ===\n",
      "train loss:1.4547864903218244\n",
      "train loss:1.2735029244935612\n",
      "train loss:1.351889588844006\n",
      "=== epoch:276, train acc:0.7233333333333334, test acc:0.5101 ===\n",
      "train loss:1.2472876589832318\n",
      "train loss:1.302461859044797\n",
      "train loss:1.4988151398658958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:277, train acc:0.72, test acc:0.5145 ===\n",
      "train loss:1.368813913196964\n",
      "train loss:1.2377141382861965\n",
      "train loss:1.2968438003598477\n",
      "=== epoch:278, train acc:0.71, test acc:0.5167 ===\n",
      "train loss:1.3226104058731365\n",
      "train loss:1.399530646050797\n",
      "train loss:1.1995433157127517\n",
      "=== epoch:279, train acc:0.71, test acc:0.5163 ===\n",
      "train loss:1.1647494779152063\n",
      "train loss:1.3459174985459972\n",
      "train loss:1.2173988386898296\n",
      "=== epoch:280, train acc:0.71, test acc:0.515 ===\n",
      "train loss:1.409816726224406\n",
      "train loss:1.3057713502106945\n",
      "train loss:1.3101535415384555\n",
      "=== epoch:281, train acc:0.7066666666666667, test acc:0.5169 ===\n",
      "train loss:1.2946427393791708\n",
      "train loss:1.3875174217095134\n",
      "train loss:1.3224129639939621\n",
      "=== epoch:282, train acc:0.7033333333333334, test acc:0.5167 ===\n",
      "train loss:1.1684399720443404\n",
      "train loss:1.2591114543761566\n",
      "train loss:1.4491116375593684\n",
      "=== epoch:283, train acc:0.71, test acc:0.5206 ===\n",
      "train loss:1.2236006518599643\n",
      "train loss:1.0931866386954723\n",
      "train loss:1.2490437430736525\n",
      "=== epoch:284, train acc:0.7133333333333334, test acc:0.5177 ===\n",
      "train loss:1.2737350932122482\n",
      "train loss:1.1218461860979796\n",
      "train loss:1.2802194690363684\n",
      "=== epoch:285, train acc:0.7133333333333334, test acc:0.5179 ===\n",
      "train loss:1.324545732254801\n",
      "train loss:1.1860014606691092\n",
      "train loss:1.306440005915135\n",
      "=== epoch:286, train acc:0.7133333333333334, test acc:0.5197 ===\n",
      "train loss:1.249760896229641\n",
      "train loss:1.2472792367911805\n",
      "train loss:1.3655578943172824\n",
      "=== epoch:287, train acc:0.71, test acc:0.5239 ===\n",
      "train loss:1.1932573542612672\n",
      "train loss:1.330632081967992\n",
      "train loss:1.3198309980023715\n",
      "=== epoch:288, train acc:0.71, test acc:0.5207 ===\n",
      "train loss:1.2631821910280852\n",
      "train loss:1.3108474512071504\n",
      "train loss:1.3860070763724845\n",
      "=== epoch:289, train acc:0.7133333333333334, test acc:0.5205 ===\n",
      "train loss:1.2077788916590722\n",
      "train loss:1.225706987743822\n",
      "train loss:1.4249064890487064\n",
      "=== epoch:290, train acc:0.7133333333333334, test acc:0.5299 ===\n",
      "train loss:1.2133081179436542\n",
      "train loss:1.2652099857570982\n",
      "train loss:1.3503332052711698\n",
      "=== epoch:291, train acc:0.7166666666666667, test acc:0.5333 ===\n",
      "train loss:1.194089080380884\n",
      "train loss:1.222047406255509\n",
      "train loss:1.3617875684707315\n",
      "=== epoch:292, train acc:0.7166666666666667, test acc:0.5366 ===\n",
      "train loss:1.20822848406113\n",
      "train loss:1.1985608101176686\n",
      "train loss:1.2826825596804607\n",
      "=== epoch:293, train acc:0.7233333333333334, test acc:0.536 ===\n",
      "train loss:1.1683542021857645\n",
      "train loss:1.1303505393190838\n",
      "train loss:1.2398327835856935\n",
      "=== epoch:294, train acc:0.7266666666666667, test acc:0.5337 ===\n",
      "train loss:1.257328588837727\n",
      "train loss:1.1400846250567231\n",
      "train loss:1.226004329768051\n",
      "=== epoch:295, train acc:0.73, test acc:0.5349 ===\n",
      "train loss:1.218270048104453\n",
      "train loss:1.251275637923745\n",
      "train loss:1.2436770811899474\n",
      "=== epoch:296, train acc:0.73, test acc:0.5445 ===\n",
      "train loss:1.2140580939627934\n",
      "train loss:1.1456262052350414\n",
      "train loss:1.1474771025145152\n",
      "=== epoch:297, train acc:0.7333333333333333, test acc:0.5538 ===\n",
      "train loss:1.091232371542832\n",
      "train loss:1.1625047330396407\n",
      "train loss:1.2446668819142104\n",
      "=== epoch:298, train acc:0.7333333333333333, test acc:0.5496 ===\n",
      "train loss:1.3261613285127225\n",
      "train loss:1.2400508156394827\n",
      "train loss:1.0537856422231606\n",
      "=== epoch:299, train acc:0.7266666666666667, test acc:0.5475 ===\n",
      "train loss:1.1106146101673324\n",
      "train loss:1.2287137679172486\n",
      "train loss:1.2491461352924869\n",
      "=== epoch:300, train acc:0.73, test acc:0.5457 ===\n",
      "train loss:1.1645185906838054\n",
      "train loss:1.2056657562631043\n",
      "train loss:1.2992713195056025\n",
      "=== epoch:301, train acc:0.7333333333333333, test acc:0.546 ===\n",
      "train loss:1.1312487403831033\n",
      "train loss:1.1326297634049236\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29522b7e348>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29523c7c6c8>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epochs')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x295247d9548>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multi_layer_net_extend import MultiLayerNetExtend\n",
    "from trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from layers import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class MultiLayerNetExtend:\n",
    "    \"\"\"완전 연결 다층 신경망(확장판)\n",
    "    가중치 감소, 드롭아웃, 배치 정규화 구현\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\n",
    "    use_dropout : 드롭아웃 사용 여부\n",
    "    dropout_ration : 드롭아웃 비율\n",
    "    use_batchNorm : 배치 정규화 사용 여부\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size_list, output_size,\n",
    "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0, \n",
    "                 use_dropout = False, dropout_ration = 0.5, use_batchnorm=False):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.use_dropout = use_dropout\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.params = {}\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.__init_weight(weight_init_std)\n",
    "\n",
    "        # 계층 생성\n",
    "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
    "        self.layers = OrderedDict()\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "                                                      self.params['b' + str(idx)])\n",
    "            if self.use_batchnorm:\n",
    "                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\n",
    "                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx-1])\n",
    "                self.layers['BatchNorm' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n",
    "                \n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "            \n",
    "            if self.use_dropout:\n",
    "                self.layers['Dropout' + str(idx)] = Dropout(dropout_ration)\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def __init_weight(self, weight_init_std):\n",
    "        \"\"\"가중치 초기화\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "        \"\"\"\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLUを使う場合に推奨される初期値\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoidを使う場合に推奨される初期値\n",
    "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key or \"BatchNorm\" in key:\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t, train_flg=False):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블 \n",
    "        \"\"\"\n",
    "        y = self.predict(x, train_flg)\n",
    "\n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "\n",
    "        return self.last_layer.forward(y, t) + weight_decay\n",
    "\n",
    "    def accuracy(self, X, T):\n",
    "        Y = self.predict(X, train_flg=False)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        if T.ndim != 1 : T = np.argmax(T, axis=1)\n",
    "\n",
    "        accuracy = np.sum(Y == T) / float(X.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, X, T):\n",
    "        \"\"\"기울기를 구한다(수치 미분).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_W = lambda W: self.loss(X, T, train_flg=True)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
    "            \n",
    "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
    "                grads['gamma' + str(idx)] = numerical_gradient(loss_W, self.params['gamma' + str(idx)])\n",
    "                grads['beta' + str(idx)] = numerical_gradient(loss_W, self.params['beta' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t, train_flg=True)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
    "                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\n",
    "                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from gradient import numerical_gradient\n",
    "from collections import OrderedDict \n",
    "\n",
    "class FourLayerNet_He2():    \n",
    "    def __init__(self, input_size, hidden_size_1,hidden_size_2, hidden_size_3 , hidden_size_4 ,output_size , weight_init_std = 0.27):\n",
    "        \n",
    "        # 가중치 초기화 / He 초기값 np.sqrt(input_size/2) 추가 \n",
    "        self.params = {} \n",
    "        self.params['W1']=weight_init_std * np.random.randn(input_size, hidden_size_1) * np.sqrt(input_size/2)\n",
    "        self.params['b1']=np.zeros(hidden_size_1)\n",
    "        self.params['W2']=weight_init_std * np.random.randn(hidden_size_1, hidden_size_2)  * np.sqrt(input_size/2)\n",
    "        self.params['b2']=np.zeros(hidden_size_2)\n",
    "        self.params['W3']=weight_init_std * np.random.randn(hidden_size_2, hidden_size_3)  * np.sqrt(input_size/2)\n",
    "        self.params['b3']=np.zeros(hidden_size_3) \n",
    "        self.params['W4']=weight_init_std * np.random.randn(hidden_size_2, hidden_size_3)  * np.sqrt(input_size/2)\n",
    "        self.params['b4']=np.zeros(hidden_size_4)  \n",
    "        self.params['W5']=weight_init_std * np.random.randn(hidden_size_4, output_size)  * np.sqrt(input_size/2) \n",
    "        self.params['b5']=np.zeros(output_size) \n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1']=Affine(self.params['W1'],self.params['b1'])\n",
    "        self.layers['Relu']= Relu()\n",
    "        self.layers['Affine2']=Affine(self.params['W2'],self.params['b2'])\n",
    "        self.layers['Relu']= Relu()\n",
    "        self.layers['Affine3']=Affine(self.params['W3'],self.params['b3'])\n",
    "        self.layers['Relu']= Relu()\n",
    "        self.layers['Affine4']=Affine(self.params['W4'],self.params['b4'])\n",
    "        self.layers['Relu']= Relu()  \n",
    "        self.layers['Affine5']=Affine(self.params['W5'],self.params['b5'])\n",
    "        self.layers['Relu']= Relu()\n",
    "        self.lastlayer = SoftmaxWithLoss()   \n",
    "        \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)           \n",
    "        return x    \n",
    "    \n",
    "    def loss(self, x, t):\n",
    "    \n",
    "        y = self.predict(x)\n",
    "        return self.lastlayer.forward(y,t)    \n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        \n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1) # y값 중에서 가장큰 값의 위치를 찍게 됨.\n",
    "        if t.ndim != 1 : t=np.argmax(t,axis =1) # 타겟 벨류가 1이 아니면 , t=np.argmax(t,axis =1)\n",
    "        accuracy = np.sum(y == t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # 경사하강법 (dw) \n",
    "    def numerical_gradient(self, x, t):\n",
    "        \n",
    "        loss_W = lambda W : self.loss(x,t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        grads['W4'] = numerical_gradient(loss_W, self.params['W4'])\n",
    "        grads['b4'] = numerical_gradient(loss_W, self.params['b4'])\n",
    "        grads['W5'] = numerical_gradient(loss_W, self.params['W5'])\n",
    "        grads['b5'] = numerical_gradient(loss_W, self.params['b5']) \n",
    "        return grads\n",
    "    \n",
    "    # 오차역전파 \n",
    "    def gradient(self, x, t):\n",
    "        \n",
    "            self.loss(x,t)\n",
    "            dout = 1\n",
    "            dout = self.lastlayer.backward(dout)            \n",
    "            layers = list(self.layers.values())\n",
    "            layers.reverse()\n",
    "            for layer in layers:\n",
    "                dout = layer.backward(dout)        \n",
    "            grads = {}\n",
    "            grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "            grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "            grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "            grads['W4'], grads['b4'] = self.layers['Affine4'].dW, self.layers['Affine4'].db\n",
    "            grads['W5'], grads['b5'] = self.layers['Affine5'].dW, self.layers['Affine5'].db  \n",
    "            return grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
